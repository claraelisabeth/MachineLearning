{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dca6400-b525-4d15-9639-3471192f9d3c",
   "metadata": {},
   "source": [
    "# Implementation of Simulated Annealing\n",
    "\n",
    "## Initial Model\n",
    "We will start with 0-Rule model, which is a simple baseline classification model. It does not use any features for prediction but instead predicts the most frequent class (mode) in the training dataset. This can serve as a baseline to compare the performance of more sophisticated models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66b76998-1885-456f-833d-ef2a8474029b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial model: DummyClassifier\n",
      "Initial parameters: (strategy='most_frequent')\n",
      "scores=0.503\n",
      "Neighborhood algorithm: GradientBoostingClassifier, parameters: [79, 0.07230287258803343]\n",
      "scores=0.9100000000000001\n",
      "Neighborhood algorithm: GradientBoostingClassifier, parameters: [49, 0.11697764672954684]\n",
      "scores=0.909\n",
      "Neighborhood algorithm: GaussianNB, parameters: []\n",
      "scores=0.8470000000000001\n",
      "Neighborhood algorithm: DecisionTreeClassifier, parameters: [6, 10]\n",
      "scores=0.893\n",
      "Neighborhood algorithm: DecisionTreeClassifier, parameters: [1, 19]\n",
      "scores=0.719\n",
      "Neighborhood algorithm: DecisionTreeClassifier, parameters: [10, 10]\n",
      "scores=0.9\n",
      "Neighborhood algorithm: GradientBoostingClassifier, parameters: [37, 0.2538584183023678]\n",
      "scores=0.9259999999999999\n",
      "Neighborhood algorithm: DecisionTreeClassifier, parameters: [3, 5]\n",
      "scores=0.873\n",
      "Neighborhood algorithm: GradientBoostingClassifier, parameters: [10, 0.2856864970748658]\n",
      "scores=0.905\n",
      "Neighborhood algorithm: DecisionTreeClassifier, parameters: [3, 9]\n",
      "scores=0.873\n",
      "Neighborhood algorithm: SVC, parameters: [7.3667593028772265, 0.493289346753636]\n",
      "scores=0.9099999999999999\n",
      "Neighborhood algorithm: SVC, parameters: [7.060168513256821, 0.6223606319006801]\n",
      "scores=0.8790000000000001\n",
      "Neighborhood algorithm: GradientBoostingClassifier, parameters: [60, 0.20982090114158608]\n",
      "scores=0.9209999999999999\n",
      "Neighborhood algorithm: GradientBoostingClassifier, parameters: [89, 0.059168214317545466]\n",
      "scores=0.909\n",
      "Neighborhood algorithm: GradientBoostingClassifier, parameters: [45, 0.08295802325804603]\n",
      "scores=0.899\n",
      "Neighborhood algorithm: GaussianNB, parameters: []\n",
      "scores=0.8470000000000001\n",
      "Neighborhood algorithm: DecisionTreeClassifier, parameters: [16, 6]\n",
      "scores=0.9030000000000001\n",
      "Neighborhood algorithm: GradientBoostingClassifier, parameters: [65, 0.1896681293593609]\n",
      "scores=0.9199999999999999\n",
      "Neighborhood algorithm: GradientBoostingClassifier, parameters: [12, 0.08786749101829705]\n",
      "scores=0.892\n",
      "Neighborhood algorithm: GradientBoostingClassifier, parameters: [73, 0.22997727148223443]\n",
      "scores=0.9259999999999999\n",
      "best_score is 0.9259999999999999\n",
      "best_solution is ['GradientBoostingClassifier', 37, 0.2538584183023678]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "\n",
    "class AutoML:\n",
    "    def __init__(self, initial_temp=100, cooling_rate=0.99, max_iterations=100, min_training_time=3600):\n",
    "        self.initial_temp = initial_temp\n",
    "        self.cooling_rate = cooling_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.min_training_time = min_training_time\n",
    "\n",
    "        self.algorithms = {\n",
    "            'DecisionTreeClassifier': {\n",
    "                'class': DecisionTreeClassifier,\n",
    "                'parameters': ['max_depth', 'min_samples_split'],\n",
    "                'ranges': [(1, 20), (2, 20)]\n",
    "            },\n",
    "            'SVC': {\n",
    "                'class': SVC,\n",
    "                'parameters': ['C', 'gamma'],\n",
    "                'ranges': [(0.01, 10), (0.01, 1)]\n",
    "            },\n",
    "            'RandomForestClassifier': {\n",
    "                'class': RandomForestClassifier,\n",
    "                'parameters': ['n_estimators', 'max_depth'],\n",
    "                'ranges': [(10, 100), (1, 20)]\n",
    "            },\n",
    "            'GradientBoostingClassifier': {\n",
    "                'class': GradientBoostingClassifier,\n",
    "                'parameters': ['n_estimators', 'learning_rate'],\n",
    "                'ranges': [(10, 100), (0.01, 0.3)]\n",
    "            },\n",
    "            'MLPClassifier': {\n",
    "                'class': MLPClassifier,\n",
    "                'parameters': ['alpha', 'learning_rate_init'],\n",
    "                'ranges': [(0.0001, 0.1), (0.0001, 0.1)]\n",
    "            },\n",
    "            'GaussianNB': { \n",
    "                'class': GaussianNB,\n",
    "                'parameters': [],  \n",
    "                'ranges': []      \n",
    "            }\n",
    "        }\n",
    "\n",
    "        self.best_solution = None\n",
    "        self.best_score = 0\n",
    "        self.model = None\n",
    "\n",
    "    def eval(self, model, X, y):\n",
    "        scores = cross_val_score(model, X, y, cv=5)\n",
    "        print('scores=' + str(np.mean(scores)))\n",
    "        return np.mean(scores)\n",
    "\n",
    "    def generate_neighborhood(self, current_solution):\n",
    "        if not isinstance(current_solution, list):\n",
    "            current_solution = [current_solution.__class__.__name__]\n",
    "    \n",
    "        algorithm_name = np.random.choice(list(self.algorithms.keys()))\n",
    "        algorithm_info = self.algorithms[algorithm_name]\n",
    "    \n",
    "        new_solution = [algorithm_name] + [None] * len(algorithm_info['parameters'])\n",
    "    \n",
    "        for i, parameter in enumerate(algorithm_info['parameters']):\n",
    "            if len(algorithm_info['ranges']) == 0:\n",
    "                continue \n",
    "    \n",
    "            low, high = algorithm_info['ranges'][i]\n",
    "    \n",
    "            if low is not None and high is not None:\n",
    "                if isinstance(high, list):  \n",
    "                    new_solution[i + 1] = np.random.choice(high)\n",
    "                elif isinstance(high, str):  \n",
    "                    current_idx = algorithm_info['ranges'][i].index(high)\n",
    "                    new_idx = (current_idx - 1) % len(algorithm_info['ranges'][i])\n",
    "                    new_solution[i + 1] = algorithm_info['ranges'][i][new_idx]\n",
    "                elif isinstance(low, int) and isinstance(high, int): \n",
    "                    new_solution[i + 1] = np.random.randint(low, high)\n",
    "                else:  \n",
    "                    new_solution[i + 1] = np.random.uniform(low, high)\n",
    "    \n",
    "        print(f\"Neighborhood algorithm: {algorithm_name}, parameters: {new_solution[1:]}\")\n",
    "        return new_solution\n",
    "\n",
    "    def create_model(self, solution):\n",
    "        algorithm_name = solution[0]\n",
    "        hyperparameters = solution[1:]\n",
    "        algorithm_class = self.algorithms[algorithm_name]['class']\n",
    "        if algorithm_name == 'MLPClassifier':\n",
    "            return algorithm_class(alpha=hyperparameters[0], learning_rate_init=hyperparameters[1])\n",
    "        elif algorithm_name == 'GaussianNB':\n",
    "            return algorithm_class()\n",
    "        else:\n",
    "            return algorithm_class(**{param: int(value) if param in ['max_depth', 'n_estimators', 'min_samples_split'] else value for param, value in zip(self.algorithms[algorithm_name]['parameters'], hyperparameters)})\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.simulated_annealing()\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"The model has not been fit yet. Please call the fit method first.\")\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def simulated_annealing(self):\n",
    "        start_time = time.time()  \n",
    "        # 0 rule model as initial model as base model\n",
    "        zero_r_model = DummyClassifier(strategy='most_frequent')\n",
    "        zero_r_model.fit(self.X, self.y) \n",
    "\n",
    "        print(f\"Initial model: DummyClassifier\")\n",
    "        print(f\"Initial parameters: (strategy='most_frequent')\")\n",
    "\n",
    "        current_solution = ['DummyClassifier']\n",
    "        current_score = self.eval(zero_r_model, self.X, self.y)\n",
    "        best_solution = current_solution\n",
    "        best_score = current_score\n",
    "    \n",
    "        temperature = self.initial_temp\n",
    "    \n",
    "        while time.time() - start_time < self.min_training_time:\n",
    "            for _ in range(10):\n",
    "                new_solution = self.generate_neighborhood(current_solution)\n",
    "                new_score = self.eval(self.create_model(new_solution), self.X, self.y)\n",
    "    \n",
    "                if new_score > current_score:\n",
    "                    current_solution = new_solution\n",
    "                    current_score = new_score\n",
    "                    if new_score > best_score:\n",
    "                        best_solution = new_solution\n",
    "                        best_score = new_score\n",
    "                else:\n",
    "                    acceptance_probability = np.exp((new_score - current_score) / temperature)\n",
    "                    if np.random.rand() < acceptance_probability:\n",
    "                        current_solution = new_solution\n",
    "                        current_score = new_score\n",
    "    \n",
    "            temperature *= self.cooling_rate\n",
    "    \n",
    "        self.best_solution = best_solution\n",
    "        self.best_score = best_score\n",
    "        self.model = self.create_model(best_solution)\n",
    "        self.model.fit(self.X, self.y)\n",
    "        print(f'best_score is {best_score}')\n",
    "        print(f'best_solution is {best_solution}')\n",
    "\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, random_state=42)\n",
    "automl = AutoML(min_training_time=10) \n",
    "automl.fit(X, y)\n",
    "predictions = automl.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1066545-30f0-4159-9661-f1a0afdcbf9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
