{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regression from Scratch\n",
    "\n",
    "Group 18 Members:\n",
    "\n",
    "- Clara Pichler, 11917694\n",
    "- Hannah Knapp, 11901857 \n",
    "- Sibel Toprakkiran, 09426341\n",
    "\n",
    "### Overview\n",
    "\n",
    "1. Bootstraping\n",
    "- `make_bootstraps(X, y, n_bootstraps=100)`\n",
    "\n",
    "2. Decision Tree Regression\n",
    "- metrcis\n",
    "- `split_dataset(X, y, feature_idx, threshold)`\n",
    "- `find_best_split(X, y)`\n",
    "- `build_tree(X, y, max_depth, min_samples_split, depth=0)`\n",
    "- `predict_tree(tree, X)`\n",
    "\n",
    "3. Random Forest Regression\n",
    "\n",
    "4. Random Forest Regression - LLM (ChatGPT)\n",
    "\n",
    "4. K-NN Regression\n",
    "\n",
    "5. Evaluation\n",
    "- Ours\n",
    "- LLM\n",
    "- sklearn\n",
    "- k-nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the data sets `airfoil_noise_data.csv` and `abalone.csv` as data frames for testing our implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3048</td>\n",
       "      <td>71.3</td>\n",
       "      <td>0.002663</td>\n",
       "      <td>126.201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3048</td>\n",
       "      <td>71.3</td>\n",
       "      <td>0.002663</td>\n",
       "      <td>125.201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3048</td>\n",
       "      <td>71.3</td>\n",
       "      <td>0.002663</td>\n",
       "      <td>125.951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3048</td>\n",
       "      <td>71.3</td>\n",
       "      <td>0.002663</td>\n",
       "      <td>127.591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3048</td>\n",
       "      <td>71.3</td>\n",
       "      <td>0.002663</td>\n",
       "      <td>127.461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     x0   x1      x2    x3        x4        y\n",
       "0   800  0.0  0.3048  71.3  0.002663  126.201\n",
       "1  1000  0.0  0.3048  71.3  0.002663  125.201\n",
       "2  1250  0.0  0.3048  71.3  0.002663  125.951\n",
       "3  1600  0.0  0.3048  71.3  0.002663  127.591\n",
       "4  2000  0.0  0.3048  71.3  0.002663  127.461"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1503 entries, 0 to 1502\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   x0      1503 non-null   int64  \n",
      " 1   x1      1503 non-null   float64\n",
      " 2   x2      1503 non-null   float64\n",
      " 3   x3      1503 non-null   float64\n",
      " 4   x4      1503 non-null   float64\n",
      " 5   y       1503 non-null   float64\n",
      "dtypes: float64(5), int64(1)\n",
      "memory usage: 70.6 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_airfol = pd.read_csv(\"./data/airfoil_noise_data.csv\")\n",
    "\n",
    "display(df_airfol.head())\n",
    "display(df_airfol.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole_weight</th>\n",
       "      <th>Shucked_weight</th>\n",
       "      <th>Viscera_weight</th>\n",
       "      <th>Shell_weight</th>\n",
       "      <th>Rings</th>\n",
       "      <th>Sex_F</th>\n",
       "      <th>Sex_I</th>\n",
       "      <th>Sex_M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.150</td>\n",
       "      <td>15</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Length  Diameter  Height  Whole_weight  Shucked_weight  Viscera_weight  \\\n",
       "0   0.455     0.365   0.095        0.5140          0.2245          0.1010   \n",
       "1   0.350     0.265   0.090        0.2255          0.0995          0.0485   \n",
       "2   0.530     0.420   0.135        0.6770          0.2565          0.1415   \n",
       "3   0.440     0.365   0.125        0.5160          0.2155          0.1140   \n",
       "4   0.330     0.255   0.080        0.2050          0.0895          0.0395   \n",
       "\n",
       "   Shell_weight  Rings  Sex_F  Sex_I  Sex_M  \n",
       "0         0.150     15  False  False   True  \n",
       "1         0.070      7  False  False   True  \n",
       "2         0.210      9   True  False  False  \n",
       "3         0.155     10  False  False   True  \n",
       "4         0.055      7  False   True  False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4175 entries, 0 to 4176\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Length          4175 non-null   float64\n",
      " 1   Diameter        4175 non-null   float64\n",
      " 2   Height          4175 non-null   float64\n",
      " 3   Whole_weight    4175 non-null   float64\n",
      " 4   Shucked_weight  4175 non-null   float64\n",
      " 5   Viscera_weight  4175 non-null   float64\n",
      " 6   Shell_weight    4175 non-null   float64\n",
      " 7   Rings           4175 non-null   int64  \n",
      " 8   Sex_F           4175 non-null   bool   \n",
      " 9   Sex_I           4175 non-null   bool   \n",
      " 10  Sex_M           4175 non-null   bool   \n",
      "dtypes: bool(3), float64(7), int64(1)\n",
      "memory usage: 305.8 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "column_names = [\"Sex\", \"Length\", \"Diameter\", \"Height\", \"Whole_weight\", \"Shucked_weight\", \"Viscera_weight\", \"Shell_weight\", \"Rings\"]\n",
    "df_abalone = pd.read_csv(\"./data/abalone.csv\", header=0, names=column_names)\n",
    "df_abalone[df_abalone.Height == 0]\n",
    "df_abalone = df_abalone[df_abalone.Height != 0]\n",
    "df_abalone = pd.get_dummies(df_abalone, columns=['Sex'], drop_first=False)\n",
    "\n",
    "display(df_abalone.head())\n",
    "display(df_abalone.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrapping is a method to create multiple subsets of the original data by sampling with replacement. Each subset is used to train one decision tree in the forest.\n",
    "\n",
    "- Introduces randomness, ensuring that trees see different views of the data.\n",
    "- Helps in reducing overfitting by decorrelating the trees.\n",
    "\n",
    "- __Sample Size__: The size of the bootstrap sample is the same as the original dataset.\n",
    "- __Replacement__: Sampling with replacement ensures diversity between bootstrapped samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bootstraps(X, y, n_bootstraps=100):\n",
    "    \n",
    "    bootstrap = []\n",
    "    sample_size = X.shape[0]\n",
    "    idx = np.arange(sample_size)\n",
    "\n",
    "    for b in range(n_bootstraps):\n",
    "        \n",
    "        sidx = np.random.choice(idx, size=sample_size, replace=True)\n",
    "        X_boot = X[sidx]\n",
    "        y_boot = y[sidx]\n",
    "        \n",
    "        bootstrap.append((X_boot, y_boot))\n",
    "        \n",
    "    return bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define some helper functions to split the data and calculate metrics like the mean squared error (MSE) to figure out the best split.\n",
    "And for the actual building of the trees we recursively split the data into smaller groups, based on feature thresholds, until a stopping condition is met (e.g., max depth or minimum samples per leaf).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y):\n",
    "    return np.mean((y - np.mean(y))**2)\n",
    "\n",
    "def mse_split(parent, y_left, y_right):\n",
    "\n",
    "    total = len(parent)\n",
    "    weighted_mse = (len(y_left) * mse(y_left) + len(y_right) * mse(y_right)) / total\n",
    "\n",
    "    return weighted_mse\n",
    "\n",
    "\n",
    "def variance_reduction_split(parent, y_left, y_right):\n",
    "\n",
    "    total = len(parent)\n",
    "    parent_variance = np.var(parent)\n",
    "    weighted_variance = (len(y_left) * np.var(y_left) + len(y_right) * np.var(y_right)) / total\n",
    "\n",
    "    return parent_variance - weighted_variance\n",
    "\n",
    "\n",
    "def mae_split(parent, y_left, y_right):\n",
    "\n",
    "    total = len(parent)\n",
    "    weighted_mae = (len(y_left) * np.mean(np.abs(y_left - np.mean(y_left))) +\n",
    "                    len(y_right) * np.mean(np.abs(y_right - np.mean(y_right)))) / total\n",
    "    \n",
    "    return weighted_mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_functions = {\n",
    "        \"mse\": mse_split,\n",
    "        \"variance_reduction\": variance_reduction_split,\n",
    "        \"mae\": mae_split,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Mean Squared Error (MSE)__\n",
    "$$\n",
    "\\text{MSE}(y) = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\bar{y})^2\n",
    "$$ \n",
    "Weighted Split Formula:\n",
    "$$\n",
    "\\frac{|y_{\\text{left}}| \\cdot \\text{MSE}(y_{\\text{left}}) + |y_{\\text{right}}| \\cdot \\text{MSE}(y_{\\text{right}})}{|y|}\n",
    "$$  \n",
    "\n",
    "- MSE minimizes the squared differences between actual and predicted values\n",
    "- It works well if the target variable is continuous \n",
    "- Sensitive to outliers since squared errors penalize large deviations more heavily \n",
    "- We will choose the split with the lowest weighted MSE!\n",
    "\n",
    "\n",
    "\n",
    "__Variance Reduction__\n",
    " \n",
    "$$\n",
    "\\text{Var}(y) - \\left( \\frac{|y_{\\text{left}}| \\cdot \\text{Var}(y_{\\text{left}}) + |y_{\\text{right}}| \\cdot \\text{Var}(y_{\\text{right}})}{|y|} \\right)\n",
    "$$ \n",
    "- Measures the decrease in variance after splitting.  \n",
    "- Effective  when capturing spread or variability in the target variable is important.  \n",
    "- Generally, less sensitive to outliers compared to MSE.\n",
    "- Choosing the split with the highest variance reduction!\n",
    "\n",
    "\n",
    "__Mean Absolute Error (MAE)__\n",
    "\n",
    "$$\n",
    "\\text{MAE}(y) = \\frac{1}{n} \\sum_{i=1}^n |y_i - \\bar{y}|\n",
    "$$ \n",
    "Weighted Split Formula: \n",
    "$$\n",
    "\\frac{|y_{\\text{left}}| \\cdot \\text{MAE}(y_{\\text{left}}) + |y_{\\text{right}}| \\cdot \\text{MAE}(y_{\\text{right}})}{|y|}\n",
    "$$  \n",
    "- MAE minimizes the __absolute__ differences between actual and predicted values.  \n",
    "- Works well when dealing with data containing outliers, as it penalizes them less compared to MSE.  \n",
    "- We will choose the split with the lowest weighted MAE!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, y, feature_idx, threshold):\n",
    "    \n",
    "    left_mask = X[:, feature_idx] <= threshold\n",
    "    right_mask = ~left_mask\n",
    "\n",
    "    return X[left_mask], X[right_mask], y[left_mask], y[right_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`split_dataset` divides X (features) and y (target) into two groups based on whether the value of a feature is less than or equal to a given threshold. This is used to evaluate potential splits during training. For example split on feature `feature_1` at value 6 creates two groups: rows where `feature_1 <= 6` and rows where `feature_1 > 6`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split(X, y, feature_subset=None, metric=\"mse\"):\n",
    "    best_feature, best_threshold = None, None\n",
    "\n",
    "    if metric in [\"variance_reduction\", \"information_gain\"]:\n",
    "        best_metric = float(\"-inf\")  \n",
    "        is_better = lambda current, best: current > best\n",
    "    else:\n",
    "        best_metric = float(\"inf\")  \n",
    "        is_better = lambda current, best: current < best\n",
    "\n",
    "    features = feature_subset if feature_subset is not None else range(X.shape[1])\n",
    "\n",
    "    for feature_idx in features:\n",
    "        thresholds = np.unique(X[:, feature_idx])\n",
    "\n",
    "        for threshold in thresholds:\n",
    "            _, _, y_left, y_right = split_dataset(X, y, feature_idx, threshold)\n",
    "\n",
    "            if len(y_left) == 0 or len(y_right) == 0:\n",
    "                continue\n",
    "\n",
    "            metric_value = metric_functions[metric](y, y_left, y_right)\n",
    "\n",
    "            if is_better(metric_value, best_metric):\n",
    "                best_metric = metric_value\n",
    "                best_feature = feature_idx\n",
    "                best_threshold = threshold\n",
    "\n",
    "    return best_feature, best_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`find_best_split` evaluates all possible splits for every feature and every threshold. It selects the best split based on the chosen metric\n",
    "\n",
    "It evaluates how well the splits separate the data into subsets that are better at differantiating the target value and making the leaf nodes more pure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(X, y, max_depth, min_samples_split, max_features, depth=0, metric=\"mse\"):\n",
    "\n",
    "    if depth >= max_depth or len(y) < min_samples_split or mse(y) == 0:\n",
    "        return np.mean(y)  \n",
    "\n",
    "    n_features = X.shape[1]\n",
    "    if max_features == 'sqrt':\n",
    "        feature_subset = np.random.choice(n_features, size=max(1, int(np.sqrt(n_features))), replace=False)\n",
    "    elif max_features == 'log2':\n",
    "        feature_subset = np.random.choice(n_features, size=max(1, int(np.log2(n_features))), replace=False)\n",
    "    else:\n",
    "        feature_subset = np.arange(n_features)\n",
    "    \n",
    "    feature_idx, threshold = find_best_split(X, y, feature_subset, metric=metric)\n",
    "    \n",
    "    if feature_idx is None:\n",
    "        return np.mean(y)  \n",
    "    \n",
    "    X_left, X_right, y_left, y_right = split_dataset(X, y, feature_idx, threshold)\n",
    "    left_subtree = build_tree(X_left, y_left, max_depth, min_samples_split, max_features, depth + 1, metric)\n",
    "    right_subtree = build_tree(X_right, y_right, max_depth, min_samples_split, max_features, depth + 1, metric)\n",
    "    \n",
    "    return {\n",
    "        \"feature_idx\": feature_idx,\n",
    "        \"threshold\": threshold,\n",
    "        \"left\": left_subtree,\n",
    "        \"right\": right_subtree,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Stopping Conditions__: Stops if the max depth is reached, if there are fewer samples than min_samples_split, or if the MSE is 0 (all values are the same).\n",
    "- __Recursive Splitting__: For each split, the function creates a left and right subtree until the stopping conditions are met.\n",
    "- __Leaf Node__: If the recursion stops, the tree stores the mean value of y for prediction.\n",
    "- __Feature Selection__: Chooses $\\sqrt{n}$ features randomly for each tree, where $n$ is the number of original features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tree(tree, X):\n",
    "\n",
    "    if isinstance(tree, dict):\n",
    "\n",
    "        feature_idx = tree[\"feature_idx\"]\n",
    "        threshold = tree[\"threshold\"]\n",
    "\n",
    "        if X[feature_idx] <= threshold:\n",
    "            return predict_tree(tree[\"left\"], X)\n",
    "        \n",
    "        else:\n",
    "            return predict_tree(tree[\"right\"], X)\n",
    "        \n",
    "    else:\n",
    "        return tree  # Leaf node\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traverses the tree based on the input features until you reach a leaf node.\n",
    "Returns the mean value of the target variable y at the leaf node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestRegressor_18:\n",
    "    \n",
    "    def __init__(self, n_trees=10, max_depth=5, min_samples_split=10, max_features='sqrt', metric='mse'):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.metric = metric\n",
    "        self.max_features = max_features\n",
    "        self.trees = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        bootstraps = make_bootstraps(X, y, n_bootstraps=self.n_trees)\n",
    "        self.trees = [\n",
    "            build_tree(X_boot, y_boot, self.max_depth, self.min_samples_split, self.max_features, metric=self.metric)\n",
    "                for X_boot, y_boot in bootstraps\n",
    "            ]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = np.array([predict_tree(tree, x) for tree in self.trees for x in X])\n",
    "        predictions = predictions.reshape(len(self.trees), len(X))\n",
    "\n",
    "        return np.mean(predictions, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use for the comparison of the methods the same holdout split with the training set containg 75% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_airfol.drop(\"y\", axis=1).values\n",
    "y = df_airfol[\"y\"].values\n",
    "\n",
    "#X = df_abalone.drop(\"Rings\", axis=1).values\n",
    "#y = df_abalone[\"Rings\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluation we use the following three methods:\n",
    "\n",
    "__Mean Squared Error (MSE)__\n",
    "\n",
    "The average of the squared differences between the predicted values and the actual values. It gives more weight to larger errors.\n",
    "\n",
    "- A smaller MSE value indicates that the model’s predictions are close to the actual values.\n",
    "- Since it's based on squared differences, large prediction errors (outliers) have a greater impact.\n",
    "- MSE is in the square of the unit of your target variable\n",
    "- Useful when large errors are particularly undesirable and need to be penalized more heavily.\n",
    "\n",
    "\n",
    "\n",
    "__Mean Absolute Error (MAE)__\n",
    "\n",
    "The average of the absolute differences between predicted values and actual values. Unlike MSE, it treats all errors equally, regardless of size.\n",
    "\n",
    "- A smaller MAE value indicates better model performance.\n",
    "- MAE is in the same unit as the target variable, making it more interpretable compared to MSE.\n",
    "- Good for understanding the typical size of prediction errors.\n",
    "- Less sensitive to outliers compared to MSE.\n",
    "\n",
    "__R-squared__\n",
    "\n",
    "The proportion of variance in the target variable that the model explains. It ranges from:\n",
    "- __1__: Perfect fit (model explains all variance in the data).\n",
    "- __0__: Model does no better than predicting the mean of the target.\n",
    "- __Negative__: Model performs worse than simply predicting the mean.\n",
    "\n",
    "\n",
    "- A higher R-squared (close to 1) indicates a good fit.\n",
    "- A low or negative R-squared suggests that your model is not capturing the relationship between the features and target effectively.\n",
    "- Helps understand how well the model explains the variability in the target variable.\n",
    "- Not ideal for measuring absolute error but useful for comparing models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_18 = RandomForestRegressor_18(n_trees=150, max_depth=10, min_samples_split=2, max_features='sqrt', metric='variance_reduction')\n",
    "rf_18.fit(X_train, y_train)\n",
    "\n",
    "predictions_18 = rf_18.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance:\n",
      "Mean Squared Error: 6.23\n",
      "Mean Absolute Error: 1.92\n",
      "R-squared: 0.87\n"
     ]
    }
   ],
   "source": [
    "mse_18 = mean_squared_error(y_test, predictions_18)\n",
    "mae_18 = mean_absolute_error(y_test, predictions_18)\n",
    "r2_18 = r2_score(y_test, predictions_18)\n",
    "\n",
    "print(\"Model Performance:\")\n",
    "print(f\"Mean Squared Error: {mse_18:.2f}\")\n",
    "print(f\"Mean Absolute Error: {mae_18:.2f}\")\n",
    "print(f\"R-squared: {r2_18:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatGPT Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeRegressor:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        data = np.concatenate((X, y.reshape(-1, 1)), axis=1)\n",
    "        self.tree = self._build_tree(data)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_row(row, self.tree) for row in X])\n",
    "\n",
    "    def _build_tree(self, data, depth=0):\n",
    "        X, y = data[:, :-1], data[:, -1]\n",
    "        num_samples, num_features = X.shape\n",
    "\n",
    "        # Stopping conditions\n",
    "        if num_samples < self.min_samples_split or (self.max_depth and depth >= self.max_depth):\n",
    "            return np.mean(y)\n",
    "\n",
    "        feature_idx, threshold = self._find_best_split(X, y)\n",
    "        if feature_idx is None:\n",
    "            return np.mean(y)\n",
    "\n",
    "        left_idxs, right_idxs = self._split(X[:, feature_idx], threshold)\n",
    "        left = self._build_tree(data[left_idxs], depth + 1)\n",
    "        right = self._build_tree(data[right_idxs], depth + 1)\n",
    "        return {\"feature_idx\": feature_idx, \"threshold\": threshold, \"left\": left, \"right\": right}\n",
    "\n",
    "    def _find_best_split(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        best_mse = float(\"inf\")\n",
    "        split_idx, split_thresh = None, None\n",
    "\n",
    "        for feature_idx in range(num_features):\n",
    "            thresholds = np.unique(X[:, feature_idx])\n",
    "            for threshold in thresholds:\n",
    "                mse = self._mean_squared_error_split(X[:, feature_idx], y, threshold)\n",
    "                if mse < best_mse:\n",
    "                    best_mse, split_idx, split_thresh = mse, feature_idx, threshold\n",
    "\n",
    "        return split_idx, split_thresh\n",
    "\n",
    "    def _mean_squared_error_split(self, feature, y, threshold):\n",
    "        left_idxs, right_idxs = self._split(feature, threshold)\n",
    "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "            return float(\"inf\")\n",
    "\n",
    "        y_left, y_right = y[left_idxs], y[right_idxs]\n",
    "        mse_left = np.mean((y_left - np.mean(y_left))**2)\n",
    "        mse_right = np.mean((y_right - np.mean(y_right))**2)\n",
    "\n",
    "        n = len(y)\n",
    "        n_left, n_right = len(y_left), len(y_right)\n",
    "        mse = (n_left / n) * mse_left + (n_right / n) * mse_right\n",
    "\n",
    "        return mse\n",
    "\n",
    "    def _split(self, feature, threshold):\n",
    "        left_idxs = np.where(feature <= threshold)[0]\n",
    "        right_idxs = np.where(feature > threshold)[0]\n",
    "        return left_idxs, right_idxs\n",
    "\n",
    "    def _predict_row(self, row, tree):\n",
    "        if isinstance(tree, dict):\n",
    "            if row[tree[\"feature_idx\"]] <= tree[\"threshold\"]:\n",
    "                return self._predict_row(row, tree[\"left\"])\n",
    "            else:\n",
    "                return self._predict_row(row, tree[\"right\"])\n",
    "        return tree\n",
    "\n",
    "\n",
    "class RFRegressor:\n",
    "    def __init__(self, n_trees=10, max_depth=None, min_samples_split=2, max_features=None):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_features = max_features\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.trees = []\n",
    "        n_samples, n_features = X.shape\n",
    "        for _ in range(self.n_trees):\n",
    "            bootstrap_idxs = np.random.choice(n_samples, n_samples, replace=True)\n",
    "            bootstrap_X = X[bootstrap_idxs]\n",
    "            bootstrap_y = y[bootstrap_idxs]\n",
    "\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
    "            if self.max_features is not None:\n",
    "                feature_idxs = np.random.choice(n_features, self.max_features, replace=False)\n",
    "                tree.fit(bootstrap_X[:, feature_idxs], bootstrap_y)\n",
    "                tree.feature_idxs = feature_idxs\n",
    "            else:\n",
    "                tree.fit(bootstrap_X, bootstrap_y)\n",
    "                tree.feature_idxs = None\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        tree_predictions = np.array([tree.predict(X[:, tree.feature_idxs] if tree.feature_idxs is not None else X)\n",
    "                                      for tree in self.trees])\n",
    "        return np.mean(tree_predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_chatgpt = RFRegressor(n_trees=150, max_depth=10, min_samples_split=2)\n",
    "rf_chatgpt.fit(X_train, y_train)\n",
    "predictions_chatgpt = rf_chatgpt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance:\n",
      "Mean Squared Error: 4.40\n",
      "Mean Absolute Error: 1.54\n",
      "R-squared: 0.91\n"
     ]
    }
   ],
   "source": [
    "mse_chatgpt = mean_squared_error(y_test, predictions_chatgpt)\n",
    "mae_chatgpt = mean_absolute_error(y_test, predictions_chatgpt)\n",
    "r2_chatgpt = r2_score(y_test, predictions_chatgpt)\n",
    "\n",
    "print(\"Model Performance:\")\n",
    "print(f\"Mean Squared Error: {mse_chatgpt:.2f}\")\n",
    "print(f\"Mean Absolute Error: {mae_chatgpt:.2f}\")\n",
    "print(f\"R-squared: {r2_chatgpt:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SkLearn Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_sklearn = RandomForestRegressor(n_estimators=50, max_depth=10, min_samples_split=2)\n",
    "rf_sklearn.fit(X_train, y_train)\n",
    "\n",
    "predictions_sklearn = rf_sklearn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor(max_depth=15)\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    \"n_estimators\": [10, 25, 50, 100, 150],\n",
    "    \"max_depth\": [5, 10, 15, 20],\n",
    "    \"min_samples_split\": [2, 3, 3, 4],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(RandomForestRegressor(), param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_rf = grid_search.best_estimator_\n",
    "print(best_rf)\n",
    "best_rf.fit(X_train, y_train)\n",
    "best_predictions = best_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance:\n",
      "Mean Squared Error: 4.15\n",
      "Mean Absolute Error: 1.52\n",
      "R-squared: 0.91\n"
     ]
    }
   ],
   "source": [
    "mse_sklearn = mean_squared_error(y_test, predictions_sklearn)\n",
    "mae_sklearn = mean_absolute_error(y_test, predictions_sklearn)\n",
    "r2_sklearn = r2_score(y_test, predictions_sklearn)\n",
    "\n",
    "print(\"Model Performance:\")\n",
    "print(f\"Mean Squared Error: {mse_sklearn:.2f}\")\n",
    "print(f\"Mean Absolute Error: {mae_sklearn:.2f}\")\n",
    "print(f\"R-squared: {r2_sklearn:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance:\n",
      "Mean Squared Error: 3.32\n",
      "Mean Absolute Error: 1.33\n",
      "R-squared: 0.91\n"
     ]
    }
   ],
   "source": [
    "mse_sklearn_best = mean_squared_error(y_test, best_predictions)\n",
    "mae_sklearn_best = mean_absolute_error(y_test, best_predictions)\n",
    "r2_sklearn_best = r2_score(y_test, predictions_sklearn)\n",
    "\n",
    "print(\"Model Performance:\")\n",
    "print(f\"Mean Squared Error: {mse_sklearn_best:.2f}\")\n",
    "print(f\"Mean Absolute Error: {mae_sklearn_best:.2f}\")\n",
    "print(f\"R-squared: {r2_sklearn:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-NN Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "knn_sklearn = KNeighborsRegressor(n_neighbors=5)\n",
    "knn_sklearn.fit(X_train, y_train)\n",
    "\n",
    "predictions_sklearn = knn_sklearn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "}\n",
    "\n",
    "knn_sklearn = KNeighborsRegressor()\n",
    "grid_search = GridSearchCV(knn_sklearn, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "predictions_sklearn_best = grid_search.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance:\n",
      "Mean Squared Error: 38.82\n",
      "Mean Absolute Error: 4.95\n",
      "R-squared: 0.19\n"
     ]
    }
   ],
   "source": [
    "mse_knn = mean_squared_error(y_test, predictions_sklearn)\n",
    "mae_knn = mean_absolute_error(y_test, predictions_sklearn)\n",
    "r2_knn = r2_score(y_test, predictions_sklearn)\n",
    "\n",
    "print(\"Model Performance:\")\n",
    "print(f\"Mean Squared Error: {mse_knn:.2f}\")\n",
    "print(f\"Mean Absolute Error: {mae_knn:.2f}\")\n",
    "print(f\"R-squared: {r2_knn:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance:\n",
      "Mean Squared Error: 37.86\n",
      "Mean Absolute Error: 4.86\n",
      "R-squared: 0.19\n"
     ]
    }
   ],
   "source": [
    "mse_knn_best = mean_squared_error(y_test, predictions_sklearn_best)\n",
    "mae_knn_best = mean_absolute_error(y_test, predictions_sklearn_best)\n",
    "r2_knn_best = r2_score(y_test, predictions_sklearn)\n",
    "\n",
    "print(\"Model Performance:\")\n",
    "print(f\"Mean Squared Error: {mse_knn_best:.2f}\")\n",
    "print(f\"Mean Absolute Error: {mae_knn_best:.2f}\")\n",
    "print(f\"R-squared: {r2_knn_best:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max Depth: Prevents trees from growing too deep, which could lead to overfitting.\n",
    "\n",
    "Min Samples Split: Controls the smallest group size allowed for further splitting, preventing unnecessary splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Key Insights from These Metrics**\n",
    "1. **MSE**:\n",
    "   - If it's high, your model is making some large errors that need to be addressed.\n",
    "   - If it's low, your model is capturing most of the relationship.\n",
    "\n",
    "2. **MAE**:\n",
    "   - Directly tells you the average prediction error. \n",
    "   - Compare it to the scale of your target variable; if MAE is relatively low, the model is performing well.\n",
    "\n",
    "3. **R²**:\n",
    "   - A high R² suggests the model explains a significant portion of the target variable's variance.\n",
    "   - If R² is low (or negative), consider if the features are truly predictive of the target or if the model is too simple/complex.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next Steps Based on Metrics**\n",
    "\n",
    "- **High MSE or MAE**:\n",
    "  - Investigate outliers, or whether the model needs better hyperparameter tuning.\n",
    "  - Consider adding more predictive features or improving feature engineering.\n",
    "\n",
    "- **Low R²**:\n",
    "  - Evaluate if features are relevant or add more features to capture the variance.\n",
    "  - Consider if the model is underfitting or overfitting:\n",
    "    - Underfitting: Increase `max_depth`, add more `n_trees`.\n",
    "    - Overfitting: Decrease `max_depth` or regularize.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
