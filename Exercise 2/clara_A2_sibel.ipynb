{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regression from Scratch\n",
    "\n",
    "Group 18 Members:\n",
    "\n",
    "- Clara Pichler, 11917694\n",
    "- Hannah Knapp, 11901857 \n",
    "- Sibel Toprakkiran, 09426341\n",
    "\n",
    "### Overview\n",
    "\n",
    "1. Bootstraping\n",
    "- `make_bootstraps(df, n_bootstraps=100)`\n",
    "\n",
    "2. Decision Tree Regression\n",
    "- `mse` or `variance reduction`\n",
    "- `split_dataset(X, y, feature_idx, threshold)`\n",
    "- `find_best_split(X, y)`\n",
    "- `build_tree(X, y, max_depth, min_samples_split, depth=0)`\n",
    "- `predict_tree(tree, X)`\n",
    "\n",
    "3. Random Forest Regression\n",
    "\n",
    "4. Random Forest Regression - LLM\n",
    "- \n",
    "\n",
    "5. Evaluation\n",
    "- Ours\n",
    "- LLM\n",
    "- sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the data set `mountains_vs_beaches_preferences.csv` as a data frame for testing our functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 52444 entries, 0 to 52443\n",
      "Data columns (total 18 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   Age                           52444 non-null  int64  \n",
      " 1   Income                        52444 non-null  int64  \n",
      " 2   Travel_Frequency              52444 non-null  int64  \n",
      " 3   Vacation_Budget               52444 non-null  int64  \n",
      " 4   Proximity_to_Mountains        52444 non-null  int64  \n",
      " 5   Proximity_to_Beaches          52444 non-null  int64  \n",
      " 6   Pets                          52444 non-null  int64  \n",
      " 7   Environmental_Concerns        52444 non-null  int64  \n",
      " 8   Preference                    52444 non-null  int64  \n",
      " 9   Location_suburban             52444 non-null  bool   \n",
      " 10  Location_urban                52444 non-null  bool   \n",
      " 11  Favorite_Season_spring        52444 non-null  bool   \n",
      " 12  Favorite_Season_summer        52444 non-null  bool   \n",
      " 13  Favorite_Season_winter        52444 non-null  bool   \n",
      " 14  Gender_male                   52444 non-null  bool   \n",
      " 15  Gender_non-binary             52444 non-null  bool   \n",
      " 16  Preferred_Activities_Encoded  52444 non-null  float64\n",
      " 17  Education_Level_Encoded       52444 non-null  int64  \n",
      "dtypes: bool(7), float64(1), int64(10)\n",
      "memory usage: 4.8 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Age                             0\n",
       "Income                          0\n",
       "Travel_Frequency                0\n",
       "Vacation_Budget                 0\n",
       "Proximity_to_Mountains          0\n",
       "Proximity_to_Beaches            0\n",
       "Pets                            0\n",
       "Environmental_Concerns          0\n",
       "Preference                      0\n",
       "Location_suburban               0\n",
       "Location_urban                  0\n",
       "Favorite_Season_spring          0\n",
       "Favorite_Season_summer          0\n",
       "Favorite_Season_winter          0\n",
       "Gender_male                     0\n",
       "Gender_non-binary               0\n",
       "Preferred_Activities_Encoded    0\n",
       "Education_Level_Encoded         0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Income</th>\n",
       "      <th>Travel_Frequency</th>\n",
       "      <th>Vacation_Budget</th>\n",
       "      <th>Proximity_to_Mountains</th>\n",
       "      <th>Proximity_to_Beaches</th>\n",
       "      <th>Pets</th>\n",
       "      <th>Environmental_Concerns</th>\n",
       "      <th>Preference</th>\n",
       "      <th>Location_suburban</th>\n",
       "      <th>Location_urban</th>\n",
       "      <th>Favorite_Season_spring</th>\n",
       "      <th>Favorite_Season_summer</th>\n",
       "      <th>Favorite_Season_winter</th>\n",
       "      <th>Gender_male</th>\n",
       "      <th>Gender_non-binary</th>\n",
       "      <th>Preferred_Activities_Encoded</th>\n",
       "      <th>Education_Level_Encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>71477</td>\n",
       "      <td>9</td>\n",
       "      <td>2477</td>\n",
       "      <td>175</td>\n",
       "      <td>267</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.500114</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69</td>\n",
       "      <td>88740</td>\n",
       "      <td>1</td>\n",
       "      <td>4777</td>\n",
       "      <td>228</td>\n",
       "      <td>190</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46</td>\n",
       "      <td>46562</td>\n",
       "      <td>0</td>\n",
       "      <td>1469</td>\n",
       "      <td>71</td>\n",
       "      <td>280</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.500114</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>99044</td>\n",
       "      <td>6</td>\n",
       "      <td>1482</td>\n",
       "      <td>31</td>\n",
       "      <td>255</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.500916</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>106583</td>\n",
       "      <td>5</td>\n",
       "      <td>516</td>\n",
       "      <td>23</td>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Income  Travel_Frequency  Vacation_Budget  Proximity_to_Mountains  \\\n",
       "0   56   71477                 9             2477                     175   \n",
       "1   69   88740                 1             4777                     228   \n",
       "2   46   46562                 0             1469                      71   \n",
       "3   32   99044                 6             1482                      31   \n",
       "4   60  106583                 5              516                      23   \n",
       "\n",
       "   Proximity_to_Beaches  Pets  Environmental_Concerns  Preference  \\\n",
       "0                   267     0                       1           1   \n",
       "1                   190     0                       1           0   \n",
       "2                   280     0                       0           1   \n",
       "3                   255     1                       0           1   \n",
       "4                   151     1                       1           0   \n",
       "\n",
       "   Location_suburban  Location_urban  Favorite_Season_spring  \\\n",
       "0              False            True                   False   \n",
       "1               True           False                   False   \n",
       "2              False            True                   False   \n",
       "3              False           False                   False   \n",
       "4               True           False                   False   \n",
       "\n",
       "   Favorite_Season_summer  Favorite_Season_winter  Gender_male  \\\n",
       "0                    True                   False         True   \n",
       "1                   False                   False         True   \n",
       "2                   False                    True        False   \n",
       "3                    True                   False        False   \n",
       "4                   False                    True        False   \n",
       "\n",
       "   Gender_non-binary  Preferred_Activities_Encoded  Education_Level_Encoded  \n",
       "0              False                      0.500114                        1  \n",
       "1              False                      0.000000                        2  \n",
       "2              False                      0.500114                        2  \n",
       "3               True                      0.500916                        0  \n",
       "4              False                      0.000000                        0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Preference\n",
       "0    39296\n",
       "1    13148\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_pref = pd.read_csv('data/mountains_vs_beaches_preferences.csv')\n",
    "\n",
    "df_pref_one_hot = pd.get_dummies(df_pref, columns=['Location', 'Favorite_Season', 'Gender'], drop_first=True)\n",
    "\n",
    "target_mean = df_pref_one_hot.groupby('Preferred_Activities')['Preference'].mean()\n",
    "df_pref_one_hot['Preferred_Activities_Encoded'] = df_pref_one_hot['Preferred_Activities'].map(target_mean)\n",
    "\n",
    "education_mapping = {'high school': 0, 'bachelor': 1, 'master': 2, 'doctorate': 3}\n",
    "df_pref_one_hot['Education_Level_Encoded'] = df_pref_one_hot['Education_Level'].map(education_mapping)\n",
    "\n",
    "df_pref_one_hot = df_pref_one_hot.drop([\"Education_Level\", \"Preferred_Activities\"], axis=1)\n",
    "\n",
    "display(df_pref_one_hot.info())\n",
    "display(df_pref_one_hot.isnull().sum())\n",
    "display(df_pref_one_hot.head())\n",
    "\n",
    "display(df_pref_one_hot['Preference'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abalone dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole_weight</th>\n",
       "      <th>Shucked_weight</th>\n",
       "      <th>Viscera_weight</th>\n",
       "      <th>Shell_weight</th>\n",
       "      <th>Rings</th>\n",
       "      <th>Sex_F</th>\n",
       "      <th>Sex_I</th>\n",
       "      <th>Sex_M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.150</td>\n",
       "      <td>15</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Length  Diameter  Height  Whole_weight  Shucked_weight  Viscera_weight  \\\n",
       "0   0.455     0.365   0.095        0.5140          0.2245          0.1010   \n",
       "1   0.350     0.265   0.090        0.2255          0.0995          0.0485   \n",
       "2   0.530     0.420   0.135        0.6770          0.2565          0.1415   \n",
       "3   0.440     0.365   0.125        0.5160          0.2155          0.1140   \n",
       "4   0.330     0.255   0.080        0.2050          0.0895          0.0395   \n",
       "\n",
       "   Shell_weight  Rings  Sex_F  Sex_I  Sex_M  \n",
       "0         0.150     15  False  False   True  \n",
       "1         0.070      7  False  False   True  \n",
       "2         0.210      9   True  False  False  \n",
       "3         0.155     10  False  False   True  \n",
       "4         0.055      7  False   True  False  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url='./data/abalone.csv'\n",
    "column_names = [\"Sex\", \"Length\", \"Diameter\", \"Height\", \"Whole_weight\", \"Shucked_weight\", \"Viscera_weight\", \"Shell_weight\", \"Rings\"]\n",
    "abalone_df = pd.read_csv(url, header=0, names=column_names)\n",
    "abalone_df[abalone_df.Height == 0]\n",
    "abalone_df = abalone_df[abalone_df.Height != 0]\n",
    "df_abalone = pd.get_dummies(abalone_df, columns=['Sex'], drop_first=False)\n",
    "df_abalone.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1503 entries, 0 to 1502\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   x0      1503 non-null   int64  \n",
      " 1   x1      1503 non-null   float64\n",
      " 2   x2      1503 non-null   float64\n",
      " 3   x3      1503 non-null   float64\n",
      " 4   x4      1503 non-null   float64\n",
      " 5   y       1503 non-null   float64\n",
      "dtypes: float64(5), int64(1)\n",
      "memory usage: 70.6 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "x0    0\n",
       "x1    0\n",
       "x2    0\n",
       "x3    0\n",
       "x4    0\n",
       "y     0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3048</td>\n",
       "      <td>71.3</td>\n",
       "      <td>0.002663</td>\n",
       "      <td>126.201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3048</td>\n",
       "      <td>71.3</td>\n",
       "      <td>0.002663</td>\n",
       "      <td>125.201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3048</td>\n",
       "      <td>71.3</td>\n",
       "      <td>0.002663</td>\n",
       "      <td>125.951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3048</td>\n",
       "      <td>71.3</td>\n",
       "      <td>0.002663</td>\n",
       "      <td>127.591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3048</td>\n",
       "      <td>71.3</td>\n",
       "      <td>0.002663</td>\n",
       "      <td>127.461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     x0   x1      x2    x3        x4        y\n",
       "0   800  0.0  0.3048  71.3  0.002663  126.201\n",
       "1  1000  0.0  0.3048  71.3  0.002663  125.201\n",
       "2  1250  0.0  0.3048  71.3  0.002663  125.951\n",
       "3  1600  0.0  0.3048  71.3  0.002663  127.591\n",
       "4  2000  0.0  0.3048  71.3  0.002663  127.461"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_airfol = pd.read_csv(\"./data/airfoil_noise_data.csv\")\n",
    "\n",
    "display(df_airfol.info())\n",
    "display(df_airfol.isnull().sum())\n",
    "display(df_airfol.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrapping is a method to create multiple subsets of the original data by sampling with replacement. Each subset is used to train one decision tree in the forest.\n",
    "\n",
    "- Introduces randomness, ensuring that trees see different views of the data.\n",
    "- Helps in reducing overfitting by decorrelating the trees.\n",
    "\n",
    "- __Sample Size__: The size of the bootstrap sample is the same as the original dataset.\n",
    "- __Replacement__: Sampling with replacement ensures diversity between bootstrapped samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bootstraps(X, y, n_bootstraps=100):\n",
    "    \n",
    "    bootstrap = []\n",
    "    sample_size = X.shape[0]\n",
    "    idx = np.arange(sample_size)\n",
    "\n",
    "    for b in range(n_bootstraps):\n",
    "        \n",
    "        sidx = np.random.choice(idx, size=sample_size, replace=True)\n",
    "        X_boot = X[sidx]\n",
    "        y_boot = y[sidx]\n",
    "        \n",
    "        bootstrap.append((X_boot, y_boot))\n",
    "        \n",
    "    return bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How our function `make_bootstraps` works:\n",
    "1. Each data point has equal probability of being selected \n",
    "2. Selecting data points from the original sample for the current bootstrap sample, with replacement! Until we reached the same size as the original data\n",
    "4. Repeating this process until we have `n_bootstraps` bootstrap samples which we save in a dictonary `dic_boot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_pref_one_hot.drop(\"Preference\", axis=1).values\n",
    "y = df_pref_one_hot[\"Preference\"].values\n",
    "\n",
    "dic_boot_pref = make_bootstraps(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define the functions to split the data and calculate metrics like the mean squared error (MSE).\n",
    "And for the actual building of the trees we recursively split the data into smaller groups, based on feature thresholds, until a stopping condition is met (e.g., max depth or minimum samples per leaf).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y):\n",
    "    return np.mean((y - np.mean(y))**2)\n",
    "\n",
    "def variance_reduction(parent, leftChild, rightChild):    \n",
    "        weight_l = len(leftChild) / len(parent)\n",
    "        weight_r = len(rightChild) / len(parent)\n",
    "        var_red = np.var(parent) - (weight_l * np.var(leftChild) + weight_r * np.var(rightChild))\n",
    "        return var_red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mse(y)` calculates how spread out the data points are from their mean. A lower MSE indicates that the values in `y` are closer to the mean, which means the split effectively reduces variability. \n",
    "\n",
    "`variance reduction(parent, leftChild, rightChild)` calculates the variance of the parent node and subtracts the weighted variance of the child nodes. Later we want to choose the split with the highest variance reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, y, feature_idx, threshold):\n",
    "    \n",
    "    left_mask = X[:, feature_idx] <= threshold\n",
    "    right_mask = ~left_mask\n",
    "\n",
    "    return X[left_mask], X[right_mask], y[left_mask], y[right_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`split_dataset` divides X (features) and y (target) into two groups based on whether the value of a feature is less than or equal to a given threshold. This is used to evaluate potential splits during training. For example split on feature `Unemployment` at value 6 creates two groups: rows where `Unemployment <= 6` and rows where `Unemployment > 6`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split(X, y, split_measure, feature_subset=None):\n",
    "\n",
    "    best_feature, best_threshold = None, None\n",
    "    best_mse = float(\"inf\")\n",
    "    best_var_red = float(\"-inf\")\n",
    "    features = feature_subset if feature_subset is not None else range(X.shape[1])\n",
    "    for feature_idx in features:\n",
    "        thresholds = np.unique(X[:, feature_idx])\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            _, _, y_left, y_right = split_dataset(X, y, feature_idx, threshold)\n",
    "            \n",
    "            if len(y_left) == 0 or len(y_right) == 0:\n",
    "                continue\n",
    "            \n",
    "            if split_measure == 'variance':\n",
    "                var_red = variance_reduction(y, y_left, y_right)\n",
    "                if var_red > best_var_red:\n",
    "                    best_var_red = var_red\n",
    "                    best_feature = feature_idx\n",
    "                    best_threshold = threshold\n",
    "                    \n",
    "            elif split_measure == 'mse':\n",
    "                mse_split = (len(y_left) * mse(y_left) + len(y_right) * mse(y_right)) / len(y)\n",
    "            \n",
    "                if mse_split < best_mse:\n",
    "                    best_mse = mse_split\n",
    "                    best_feature = feature_idx\n",
    "                    best_threshold = threshold\n",
    "                \n",
    "    return best_feature, best_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`find_best_split` evaluates all possible splits for every feature and every threshold. It selects the split that minimizes the weighted MSE of the left and right groups:\n",
    "$$\n",
    "  \\text{Weighted MSE} = \\frac{n_{\\text{left}}}{n_{\\text{total}}} \\cdot \\text{MSE(left)} + \\frac{n_{\\text{right}}}{n_{\\text{total}}} \\cdot \\text{MSE(right)}\n",
    "$$\n",
    "This ensures splits are chosen based on their ability to reduce error. \n",
    "The output is the `best_feature` (column index of the splitting feature) and the `best_threshold` (value of the feature where the split happens).\n",
    "\n",
    "The same can be done with the method of variance reduction we want to maximize the variance reduction.\n",
    "\n",
    "$$\n",
    "  \\text{Variance Reduction} = \\text{Variance(parent)} - \\big[\\frac{n_{\\text{left}}}{n_{\\text{total}}} \\cdot \\text{Variance(left)} + \\frac{n_{\\text{right}}}{n_{\\text{total}}} \\cdot \\text{Variance(right)}\\big]\n",
    "$$\n",
    "\n",
    "It evaluates how well the splits separate the data into subsets that are better at differantiating the target value and making the leaf nodes more pure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(X, y, max_depth, min_samples_split,max_features, split_measure, depth=0):\n",
    "\n",
    "    if depth >= max_depth or len(y) < min_samples_split or mse(y) == 0:\n",
    "        return np.mean(y)  \n",
    "\n",
    "    n_features = X.shape[1]\n",
    "    \n",
    "    if max_features == 'sqrt':\n",
    "        feature_subset = np.random.choice(n_features, size=max(1, int(np.sqrt(n_features))), replace=False)\n",
    "    elif max_features == 'log2':\n",
    "        feature_subset = np.random.choice(n_features, size=max(1, int(np.log2(n_features))), replace=False)\n",
    "    else:\n",
    "        feature_subset = np.arange(n_features)\n",
    "        \n",
    "    feature_idx, threshold = find_best_split(X, y, split_measure, feature_subset)\n",
    "    \n",
    "    if feature_idx is None:\n",
    "        return np.mean(y)  \n",
    "    \n",
    "    X_left, X_right, y_left, y_right = split_dataset(X, y, feature_idx, threshold)\n",
    "    left_subtree = build_tree(X_left, y_left, max_depth, min_samples_split,max_features, split_measure, depth + 1)\n",
    "    right_subtree = build_tree(X_right, y_right, max_depth, min_samples_split,max_features, split_measure,depth + 1)\n",
    "    \n",
    "    return {\n",
    "        \"feature_idx\": feature_idx,\n",
    "        \"threshold\": threshold,\n",
    "        \"left\": left_subtree,\n",
    "        \"right\": right_subtree,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Stopping Conditions__: Stops if the max depth is reached, if there are fewer samples than min_samples_split, or if the MSE is 0 (all values are the same).\n",
    "- __Recursive Splitting__: For each split, the function creates a left and right subtree until the stopping conditions are met.\n",
    "- __Leaf Node__: If the recursion stops, the tree stores the mean value of y for prediction.\n",
    "- __Feature Selection__: Chooses $\\sqrt{n}$ features randomly for each tree, where $n$ is the number of original features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tree(tree, X):\n",
    "    if isinstance(tree, dict):\n",
    "\n",
    "        feature_idx = tree[\"feature_idx\"]\n",
    "        threshold = tree[\"threshold\"]\n",
    "        if X[feature_idx] <= threshold:\n",
    "            return predict_tree(tree[\"left\"], X)\n",
    "        \n",
    "        else:\n",
    "            return predict_tree(tree[\"right\"], X)\n",
    "        \n",
    "    else:\n",
    "        return tree  # Leaf node\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traverses the tree based on the input features until you reach a leaf node.\n",
    "Returns the mean value of the target variable y at the leaf node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestRegressor_18:\n",
    "    \n",
    "    def __init__(self, n_trees=10, max_depth=5, min_samples_split=10, max_features='sqrt', split_measure='mse'):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.split_measure = split_measure\n",
    "        self.max_features = max_features\n",
    "        self.trees = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        bootstraps = make_bootstraps(X, y, n_bootstraps=self.n_trees)\n",
    "        self.trees = [\n",
    "            build_tree(X_boot, y_boot, self.max_depth, self.min_samples_split, self.max_features, self.split_measure)\n",
    "            for X_boot, y_boot in bootstraps\n",
    "        ]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = np.array([predict_tree(tree, x) for tree in self.trees for x in X])\n",
    "        predictions = predictions.reshape(len(self.trees), len(X))\n",
    "\n",
    "        return np.mean(predictions, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use for the comparison of the methods the same holdout split with the training set containg 75% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = df_pref_one_hot.drop(\"Preference\", axis=1).values\n",
    "#y = df_pref_one_hot[\"Preference\"].values\n",
    "\n",
    "X_airfol = df_airfol.drop(\"y\", axis=1).values\n",
    "y_airfol = df_airfol[\"y\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_airfol, y_airfol, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.58 0.455 0.13 ... False False True]\n",
      " [0.59 0.45 0.155 ... False False True]\n",
      " [0.295 0.22 0.07 ... False True False]\n",
      " ...\n",
      " [0.52 0.43 0.15 ... True False False]\n",
      " [0.575 0.46 0.15 ... True False False]\n",
      " [0.595 0.475 0.16 ... True False False]]\n",
      "[[0.59 0.46 0.155 ... False False True]\n",
      " [0.31 0.245 0.095 ... False False True]\n",
      " [0.37 0.28 0.09 ... False True False]\n",
      " ...\n",
      " [0.465 0.39 0.11 ... True False False]\n",
      " [0.415 0.305 0.12 ... False True False]\n",
      " [0.455 0.365 0.095 ... False False True]]\n",
      "[ 8  8  6 ... 11  9  6]\n",
      "[15  7 11 ... 13  7 15]\n"
     ]
    }
   ],
   "source": [
    "X_abalone = df_abalone.drop(\"Rings\", axis=1).values\n",
    "y_abalone = df_abalone[\"Rings\"].values\n",
    "\n",
    "X_train_abalone, X_test_abalone, y_train_abalone, y_test_abalone = train_test_split(X_abalone, y_abalone, test_size=0.25, random_state=42)\n",
    "print(X_train_abalone)\n",
    "print(X_test_abalone)\n",
    "print(y_train_abalone)\n",
    "print(y_test_abalone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluation we use the following three methods:\n",
    "\n",
    "__Mean Squared Error (MSE)__\n",
    "\n",
    "The average of the squared differences between the predicted values and the actual values. It gives more weight to larger errors.\n",
    "\n",
    "- A smaller MSE value indicates that the modelâ€™s predictions are close to the actual values.\n",
    "- Since it's based on squared differences, large prediction errors (outliers) have a greater impact.\n",
    "- MSE is in the square of the unit of your target variable\n",
    "- Useful when large errors are particularly undesirable and need to be penalized more heavily.\n",
    "\n",
    "\n",
    "\n",
    "__Mean Absolute Error (MAE)__\n",
    "\n",
    "The average of the absolute differences between predicted values and actual values. Unlike MSE, it treats all errors equally, regardless of size.\n",
    "\n",
    "- A smaller MAE value indicates better model performance.\n",
    "- MAE is in the same unit as the target variable, making it more interpretable compared to MSE.\n",
    "- Good for understanding the typical size of prediction errors.\n",
    "- Less sensitive to outliers compared to MSE.\n",
    "\n",
    "__R-squared__\n",
    "\n",
    "The proportion of variance in the target variable that the model explains. It ranges from:\n",
    "- __1__: Perfect fit (model explains all variance in the data).\n",
    "- __0__: Model does no better than predicting the mean of the target.\n",
    "- __Negative__: Model performs worse than simply predicting the mean.\n",
    "\n",
    "\n",
    "- A higher R-squared (close to 1) indicates a good fit.\n",
    "- A low or negative R-squared suggests that your model is not capturing the relationship between the features and target effectively.\n",
    "- Helps understand how well the model explains the variability in the target variable.\n",
    "- Not ideal for measuring absolute error but useful for comparing models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([126.52880034, 123.61320053, 123.2279974 , 132.64856257,\n",
       "       130.96797284, 128.77947793, 124.51729911, 129.53864314,\n",
       "       129.5206619 , 127.80398286, 128.84572444, 115.58702656,\n",
       "       129.01106537, 129.94219394, 127.47733826, 113.61797595,\n",
       "       127.64176105, 127.69568402, 128.08258963, 128.70315844,\n",
       "       121.87704828, 127.2426254 , 114.95276071, 127.76992907,\n",
       "       124.74893164, 125.25544326, 128.9457459 , 127.64176105,\n",
       "       114.6380739 , 126.01599378, 125.15231021, 119.81154658,\n",
       "       127.36691285, 121.87836788, 118.54270628, 129.94651535,\n",
       "       129.83893134, 129.06782561, 121.02442678, 118.473585  ,\n",
       "       125.28653535, 131.0083402 , 128.23904789, 124.0186438 ,\n",
       "       127.18932393, 131.15557401, 126.28768381, 118.73722903,\n",
       "       122.11514906, 131.66503385, 129.46405083, 115.74797821,\n",
       "       129.16525722, 125.99066611, 127.89759309, 120.67757555,\n",
       "       121.15638705, 127.37091379, 126.79233697, 118.16531979,\n",
       "       125.72453135, 131.21405712, 127.80636406, 129.73327944,\n",
       "       118.60637729, 124.24026869, 125.54938124, 123.07370487,\n",
       "       127.2743683 , 126.97355578, 127.43537212, 128.94012259,\n",
       "       126.97355578, 117.48217512, 128.98394214, 124.65398915,\n",
       "       127.35320587, 129.14029092, 132.19039856, 131.6788356 ,\n",
       "       124.51729911, 120.90190657, 126.79233697, 123.95568929,\n",
       "       125.26698307, 118.31556207, 115.37548199, 128.51806067,\n",
       "       115.67518115, 123.11551912, 117.76140187, 123.48530405,\n",
       "       124.58628049, 130.59561268, 116.31310268, 116.40037431,\n",
       "       118.64925784, 117.62122694, 129.01128127, 121.35815242,\n",
       "       126.18818372, 113.70709726, 128.16187005, 123.9571863 ,\n",
       "       124.91982107, 124.78640309, 116.45248818, 123.59456715,\n",
       "       128.76758377, 125.01740429, 119.31092234, 129.27102358,\n",
       "       129.19827413, 126.97903439, 125.37491299, 130.54414589,\n",
       "       121.8186753 , 115.79984974, 127.38876242, 119.96649237,\n",
       "       127.48541838, 128.4225214 , 127.77558975, 126.97355578,\n",
       "       113.45116944, 126.27510545, 127.08514252, 120.79577322,\n",
       "       127.25458715, 111.87583077, 128.8239295 , 127.19443976,\n",
       "       114.49968293, 121.20960152, 128.49505405, 127.1429031 ,\n",
       "       127.48649088, 118.78051696, 127.3294291 , 127.26622923,\n",
       "       127.13338673, 128.64560619, 124.83547529, 127.10280557,\n",
       "       124.74817829, 126.35454148, 126.79953515, 127.34449544,\n",
       "       129.51932284, 126.97355578, 127.03291735, 130.50391075,\n",
       "       118.29576136, 130.22613609, 120.03363066, 128.43301396,\n",
       "       127.15398668, 127.88397176, 132.76875317, 124.90917779,\n",
       "       126.0306992 , 117.83728873, 124.18100686, 125.09971365,\n",
       "       122.57156614, 127.35320587, 127.92360794, 118.77591043,\n",
       "       125.26186342, 130.63870158, 122.79822487, 127.56925271,\n",
       "       127.47147846, 127.69568402, 119.93484804, 119.60500131,\n",
       "       126.64958401, 123.97769658, 125.03168176, 123.93636654,\n",
       "       129.3215206 , 115.81584559, 116.03101969, 129.22688338,\n",
       "       121.98715883, 126.52880034, 113.91413881, 128.95126332,\n",
       "       115.66407811, 128.34293624, 124.03891771, 117.83254656,\n",
       "       118.42448921, 126.22816342, 126.84313477, 128.93796128,\n",
       "       123.45856626, 128.57515128, 127.55154877, 116.56114325,\n",
       "       126.43498485, 124.61433341, 123.97339592, 127.60838298,\n",
       "       125.87460559, 127.15398668, 124.27007375, 130.48729557,\n",
       "       124.31576788, 126.08045824, 124.60710462, 129.60073829,\n",
       "       116.88485237, 129.66635129, 124.33544796, 120.96233475,\n",
       "       126.69448026, 128.13248694, 127.37091379, 117.00885623,\n",
       "       130.62910765, 118.47426113, 126.97355578, 127.96605944,\n",
       "       125.9781886 , 125.98379961, 126.97355578, 128.70406006,\n",
       "       126.79233697, 126.90905699, 125.93620201, 130.31975287,\n",
       "       123.82877803, 122.23262724, 123.71162051, 127.60838298,\n",
       "       131.50063382, 129.36521644, 124.57829398, 120.74177994,\n",
       "       114.13953118, 128.12632338, 116.81323881, 118.0512518 ,\n",
       "       129.49093526, 127.92360794, 121.11040063, 127.08966924,\n",
       "       117.20889167, 128.54435962, 117.21738297, 110.26782904,\n",
       "       125.11161254, 126.59329913, 131.38241555, 124.46567423,\n",
       "       126.98911737, 114.62498359, 123.91871994, 122.10752145,\n",
       "       124.31879145, 127.87797389, 126.08367199, 129.57223859,\n",
       "       132.29571088, 124.36761749, 128.96197243, 126.58527812,\n",
       "       127.22643223, 125.4968066 , 130.58169742, 123.94339821,\n",
       "       124.15973759, 129.39467184, 127.3294291 , 128.66417126,\n",
       "       130.89523745, 125.01479954, 125.93620201, 123.63934402,\n",
       "       125.91990688, 119.88429606, 124.48790489, 123.46371482,\n",
       "       130.26101245, 126.97355578, 122.4789339 , 119.51557919,\n",
       "       128.34590588, 126.52880034, 120.25584788, 129.37250158,\n",
       "       123.11125756, 114.90792406, 124.12937134, 124.74817829,\n",
       "       132.6916835 , 116.4733623 , 128.55278494, 127.97189652,\n",
       "       129.67611618, 130.16977842, 129.36672269, 126.94237027,\n",
       "       123.88143341, 127.43711018, 117.6334012 , 118.55574323,\n",
       "       128.35576238, 128.66417126, 118.08276778, 127.14573004,\n",
       "       123.95476925, 127.08514252, 118.76083317, 129.3245447 ,\n",
       "       130.94563768, 122.74723086, 126.79953515, 119.24647281,\n",
       "       126.44806026, 127.36102956, 128.22304332, 120.94855315,\n",
       "       129.7971221 , 118.34150768, 127.13144803, 126.90905699,\n",
       "       116.24487321, 125.31498999, 126.69448026, 114.71921653,\n",
       "       128.90358107, 127.00253544, 119.19110309, 128.91312656,\n",
       "       128.88717813, 126.97355578, 127.20940382, 127.13351602,\n",
       "       130.13959878, 127.45286056, 122.60560362, 127.50740449,\n",
       "       120.59514299, 128.87292503, 132.83891731, 126.59329913,\n",
       "       126.66665239, 127.8618363 , 125.09412519, 126.52880034,\n",
       "       128.4262194 , 126.17946622, 132.47083483, 127.8618363 ,\n",
       "       129.41108702, 116.70882437, 124.91616365, 124.50233546,\n",
       "       121.58536491, 127.88047429, 115.93851244, 124.96602872,\n",
       "       123.95986233, 121.47912311, 127.08514252, 124.89417109,\n",
       "       126.90905699, 127.08252192, 126.79233697, 128.96197243,\n",
       "       116.2511832 , 124.93280334, 122.70450068, 126.36871786])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_18 = RandomForestRegressor_18(n_trees=50, max_depth=5, min_samples_split=2,max_features='sqrt',split_measure='variance')\n",
    "rf_18.fit(X_train, y_train)\n",
    "\n",
    "predictions_18 = rf_18.predict(X_test)\n",
    "predictions_18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time run for mountains vs. beaches dataset: 34m 49s\n",
    "\n",
    "Time run for Airfoil dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance:\n",
      "Mean Squared Error: 15.32\n",
      "Mean Absolute Error: 3.16\n",
      "R-squared: 0.68\n"
     ]
    }
   ],
   "source": [
    "mse_18 = mean_squared_error(y_test, predictions_18)\n",
    "mae_18 = mean_absolute_error(y_test, predictions_18)\n",
    "r2_18 = r2_score(y_test, predictions_18)\n",
    "\n",
    "print(\"Model Performance:\")\n",
    "print(f\"Mean Squared Error: {mse_18:.2f}\")\n",
    "print(f\"Mean Absolute Error: {mae_18:.2f}\")\n",
    "print(f\"R-squared: {r2_18:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting abalone dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.91220657  7.45386195  6.93546485 ... 11.27165007  7.63471381\n",
      "  8.93261369]\n"
     ]
    }
   ],
   "source": [
    "from time import process_time, time\n",
    "\n",
    "rf_18_abalone = RandomForestRegressor_18(n_trees=20, max_depth=5, min_samples_split=3, split_measure='variance')\n",
    "#rf_18_abalone = RandomForestRegressor_18(n_trees=20, max_depth=5, min_samples_split=3, split_measure='variance')\n",
    "rf_18_abalone.fit(X_train_abalone, y_train_abalone)\n",
    "predictions_18_abalone = rf_18_abalone.predict(X_test_abalone)\n",
    "print(predictions_18_abalone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance:\n",
      "Mean Squared Error: 5.58\n",
      "Mean Absolute Error: 1.64\n",
      "R-squared: 0.49\n"
     ]
    }
   ],
   "source": [
    "mse_18 = mean_squared_error(y_test_abalone, predictions_18_abalone)\n",
    "mae_18 = mean_absolute_error(y_test_abalone, predictions_18_abalone)\n",
    "r2_18 = r2_score(y_test_abalone, predictions_18_abalone)\n",
    "\n",
    "print(\"Model Performance:\")\n",
    "print(f\"Mean Squared Error: {mse_18:.2f}\")\n",
    "print(f\"Mean Absolute Error: {mae_18:.2f}\")\n",
    "print(f\"R-squared: {r2_18:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics for abalone dataset:\n",
    "\n",
    "Model Performance:\n",
    "Mean Squared Error: \n",
    "Mean Absolute Error: \n",
    "R-squared: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SkLearn Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_sklearn = RandomForestRegressor(n_estimators=20, max_depth = 5, min_samples_split = 3)\n",
    "rf_sklearn.fit(X_train, y_train)\n",
    "\n",
    "predictions_sklearn = rf_sklearn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_sklearn = mean_squared_error(y_test, predictions_sklearn)\n",
    "mae_sklearn = mean_absolute_error(y_test, predictions_sklearn)\n",
    "r2_sklearn = r2_score(y_test, predictions_sklearn)\n",
    "\n",
    "print(\"Model Performance:\")\n",
    "print(f\"Mean Squared Error: {mse_sklearn:.2f}\")\n",
    "print(f\"Mean Absolute Error: {mae_sklearn:.2f}\")\n",
    "print(f\"R-squared: {r2_sklearn:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics for mountains vs. beaches dataset:\n",
    "\n",
    "Model Performance:\n",
    "Mean Squared Error: 0.01\n",
    "Mean Absolute Error: 0.04\n",
    "R-squared: 0.94\n",
    "\n",
    "Metrics run for Airfoil dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatGPT Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeRegressor:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        data = np.concatenate((X, y.reshape(-1, 1)), axis=1)\n",
    "        self.tree = self._build_tree(data)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_row(row, self.tree) for row in X])\n",
    "\n",
    "    def _build_tree(self, data, depth=0):\n",
    "        X, y = data[:, :-1], data[:, -1]\n",
    "        num_samples, num_features = X.shape\n",
    "\n",
    "        # Stopping conditions\n",
    "        if num_samples < self.min_samples_split or (self.max_depth and depth >= self.max_depth):\n",
    "            return np.mean(y)\n",
    "\n",
    "        feature_idx, threshold = self._find_best_split(X, y)\n",
    "        if feature_idx is None:\n",
    "            return np.mean(y)\n",
    "\n",
    "        left_idxs, right_idxs = self._split(X[:, feature_idx], threshold)\n",
    "        left = self._build_tree(data[left_idxs], depth + 1)\n",
    "        right = self._build_tree(data[right_idxs], depth + 1)\n",
    "        return {\"feature_idx\": feature_idx, \"threshold\": threshold, \"left\": left, \"right\": right}\n",
    "\n",
    "    def _find_best_split(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        best_mse = float(\"inf\")\n",
    "        split_idx, split_thresh = None, None\n",
    "\n",
    "        for feature_idx in range(num_features):\n",
    "            thresholds = np.unique(X[:, feature_idx])\n",
    "            for threshold in thresholds:\n",
    "                mse = self._mean_squared_error_split(X[:, feature_idx], y, threshold)\n",
    "                if mse < best_mse:\n",
    "                    best_mse, split_idx, split_thresh = mse, feature_idx, threshold\n",
    "\n",
    "        return split_idx, split_thresh\n",
    "\n",
    "    def _mean_squared_error_split(self, feature, y, threshold):\n",
    "        left_idxs, right_idxs = self._split(feature, threshold)\n",
    "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "            return float(\"inf\")\n",
    "\n",
    "        y_left, y_right = y[left_idxs], y[right_idxs]\n",
    "        mse_left = np.mean((y_left - np.mean(y_left))**2)\n",
    "        mse_right = np.mean((y_right - np.mean(y_right))**2)\n",
    "\n",
    "        n = len(y)\n",
    "        n_left, n_right = len(y_left), len(y_right)\n",
    "        mse = (n_left / n) * mse_left + (n_right / n) * mse_right\n",
    "\n",
    "        return mse\n",
    "\n",
    "    def _split(self, feature, threshold):\n",
    "        left_idxs = np.where(feature <= threshold)[0]\n",
    "        right_idxs = np.where(feature > threshold)[0]\n",
    "        return left_idxs, right_idxs\n",
    "\n",
    "    def _predict_row(self, row, tree):\n",
    "        if isinstance(tree, dict):\n",
    "            if row[tree[\"feature_idx\"]] <= tree[\"threshold\"]:\n",
    "                return self._predict_row(row, tree[\"left\"])\n",
    "            else:\n",
    "                return self._predict_row(row, tree[\"right\"])\n",
    "        return tree\n",
    "\n",
    "\n",
    "class RandomForestRegressor:\n",
    "    def __init__(self, n_trees=10, max_depth=None, min_samples_split=2, max_features=None):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_features = max_features\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.trees = []\n",
    "        n_samples, n_features = X.shape\n",
    "        for _ in range(self.n_trees):\n",
    "            bootstrap_idxs = np.random.choice(n_samples, n_samples, replace=True)\n",
    "            bootstrap_X = X[bootstrap_idxs]\n",
    "            bootstrap_y = y[bootstrap_idxs]\n",
    "\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
    "            if self.max_features is not None:\n",
    "                feature_idxs = np.random.choice(n_features, self.max_features, replace=False)\n",
    "                tree.fit(bootstrap_X[:, feature_idxs], bootstrap_y)\n",
    "                tree.feature_idxs = feature_idxs\n",
    "            else:\n",
    "                tree.fit(bootstrap_X, bootstrap_y)\n",
    "                tree.feature_idxs = None\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        tree_predictions = np.array([tree.predict(X[:, tree.feature_idxs] if tree.feature_idxs is not None else X)\n",
    "                                      for tree in self.trees])\n",
    "        return np.mean(tree_predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_chatgpt = RandomForestRegressor(n_trees=20, max_depth=5, min_samples_split=3)\n",
    "rf_chatgpt.fit(X_train, y_train)\n",
    "predictions_chatgpt = rf_chatgpt.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time run for mountains vs. beaches dataset:\n",
    "\n",
    "Time run for Airfoil dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_chatgpt = mean_squared_error(y_test, predictions_chatgpt)\n",
    "mae_chatgpt = mean_absolute_error(y_test, predictions_chatgpt)\n",
    "r2_chatgpt = r2_score(y_test, predictions_chatgpt)\n",
    "\n",
    "print(\"Model Performance:\")\n",
    "print(f\"Mean Squared Error: {mse_chatgpt:.2f}\")\n",
    "print(f\"Mean Absolute Error: {mae_chatgpt:.2f}\")\n",
    "print(f\"R-squared: {r2_chatgpt:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics for mountains vs. beaches dataset:\n",
    "\n",
    "Metrics run for Airfoil dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max Depth: Prevents trees from growing too deep, which could lead to overfitting.\n",
    "\n",
    "Min Samples Split: Controls the smallest group size allowed for further splitting, preventing unnecessary splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Key Insights from These Metrics**\n",
    "1. **MSE**:\n",
    "   - If it's high, your model is making some large errors that need to be addressed.\n",
    "   - If it's low, your model is capturing most of the relationship.\n",
    "\n",
    "2. **MAE**:\n",
    "   - Directly tells you the average prediction error. \n",
    "   - Compare it to the scale of your target variable; if MAE is relatively low, the model is performing well.\n",
    "\n",
    "3. **RÂ²**:\n",
    "   - A high RÂ² suggests the model explains a significant portion of the target variable's variance.\n",
    "   - If RÂ² is low (or negative), consider if the features are truly predictive of the target or if the model is too simple/complex.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next Steps Based on Metrics**\n",
    "\n",
    "- **High MSE or MAE**:\n",
    "  - Investigate outliers, or whether the model needs better hyperparameter tuning.\n",
    "  - Consider adding more predictive features or improving feature engineering.\n",
    "\n",
    "- **Low RÂ²**:\n",
    "  - Evaluate if features are relevant or add more features to capture the variance.\n",
    "  - Consider if the model is underfitting or overfitting:\n",
    "    - Underfitting: Increase `max_depth`, add more `n_trees`.\n",
    "    - Overfitting: Decrease `max_depth` or regularize.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
