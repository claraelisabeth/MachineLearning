{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Machine Learning from Scratch\n",
    "\n",
    "Group 18 Members:\n",
    "\n",
    "- Clara Pichler, 11917694\n",
    "- Hannah Knapp, 11901857 \n",
    "- Sibel Toprakkiran, 09426341\n",
    "\n",
    "### Overview\n",
    "\n",
    "1. Data Set Splitting and Preprocessing\n",
    "\n",
    "2. generate neighborhood\n",
    "- `generate_neighborhood(self, current_solution)`\n",
    "\n",
    "3. create model\n",
    "- `create_model(self, solution)`\n",
    "\n",
    "4. simulated annealing\n",
    "- `simulated_annealing(self)`\n",
    "\n",
    "5. Comparison with two state of the art AutoML systems\n",
    "- auto-sklearn \n",
    "- TPOT\n",
    "\n",
    "6. Evaluation\n",
    "- Iris Dataset\n",
    "- Congressional Voting Dataset\n",
    "- gym session tracking Dataset\n",
    "- Abalone Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVC\n",
    "import time\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import LinearRegression, Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data sets\n",
    "\n",
    "- Iris Dataset\n",
    "- Congressional Voting Dataset\n",
    "- gym session tracking Dataset\n",
    "- Abalone Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "iris_data = pd.DataFrame(data= np.c_[iris['data'], iris['target']], columns= iris['feature_names'] + ['target'])\n",
    "iris_data['target'] = iris_data['target'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
    "\n",
    "df_voting = pd.read_csv('data/CongressionalVotingID.shuf.lrn.csv')\n",
    "\n",
    "df_airfoil = pd.read_csv(\"data/airfoil_noise_data.csv\")\n",
    "\n",
    "url='./data/abalone.csv'\n",
    "column_names = [\"Sex\", \"Length\", \"Diameter\", \"Height\", \"Whole_weight\", \"Shucked_weight\", \"Viscera_weight\", \"Shell_weight\", \"Rings\"]\n",
    "abalone_df = pd.read_csv(url, header=0, names=column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hannahknapp/Library/Python/3.10/lib/python/site-packages/sklearn/impute/_iterative.py:895: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "df_voting = df_voting.replace({\"democrat\": 0,\"republican\": 1,\"n\": 0,\"y\": 1,\"unknown\": np.nan})\n",
    "df_voting = df_voting.drop(columns=['ID'])\n",
    "\n",
    "imp = IterativeImputer(max_iter=10, random_state=0)\n",
    "df_voting = pd.DataFrame(imp.fit_transform(df_voting), columns=df_voting.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test-validation-train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_iris = iris_data.drop(['target'], axis=1)\n",
    "y_iris = iris_data['target']\n",
    "\n",
    "X_train_iris, X_temp, y_train_iris, y_temp = train_test_split(X_iris, y_iris, test_size=0.4, random_state=42)\n",
    "X_val_iris, X_test_iris, y_val_iris, y_test_iris = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_voting = df_voting.drop(['class'], axis=1)\n",
    "y_voting = df_voting['class']\n",
    "\n",
    "X_train_voting, X_temp, y_train_voting, y_temp = train_test_split(X_voting, y_voting, test_size=0.4, random_state=42)\n",
    "X_val_voting, X_test_voting, y_val_voting, y_test_voting = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_airfoil = df_airfoil.drop(['y'], axis=1)\n",
    "y_airfoil = df_airfoil['y']\n",
    "\n",
    "X_train_airfoil, X_temp, y_train_airfoil, y_temp = train_test_split(X_airfoil, y_airfoil, test_size=0.4, random_state=42)\n",
    "X_val_airfoil, X_test_airfoil, y_val_airfoil, y_test_airfoil = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_abalone = abalone_df.drop(['Sex'], axis=1)\n",
    "y_abalone = abalone_df['Sex']\n",
    "\n",
    "X_train_abalone, X_temp, y_train_abalone, y_temp = train_test_split(X_abalone, y_abalone, test_size=0.4, random_state=42)\n",
    "X_val_abalone, X_test_abalone, y_val_abalone, y_test_abalone = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "- MLP\n",
    "- RF\n",
    "- KNN \n",
    "- SVM\n",
    "- AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoML_18:\n",
    "    def __init__(self, initial_temp=100, cooling_rate=0.99, max_iterations=100, min_training_time=3600, classifier = True):\n",
    "        self.initial_temp = initial_temp\n",
    "        self.cooling_rate = cooling_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.min_training_time = min_training_time\n",
    "        self.classifier = classifier\n",
    "        self.algorithms_classifier = {\n",
    "            \"MLPClassifier\": {\n",
    "                \"class\": MLPClassifier,\n",
    "                \"parameters\": [\"max_iter\", \"activation\", \"solver\", \"alpha\"],\n",
    "                \"values\": [[1000, 2000, 3000], ['relu', 'tanh', 'logistic'], ['adam', 'sgd'], [0.0001, 0.001, 0.01]]\n",
    "            },\n",
    "            \"RandomForestClassifier\": {\n",
    "                \"class\": RandomForestClassifier,\n",
    "                \"parameters\": [\"n_estimators\", \"max_depth\", \"min_samples_split\", \"max_features\", \"criterion\"],\n",
    "                \"values\": [[10, 25, 50, 100, 150], [5, 10, 15], [2, 3, 3, 4], ['sqrt', 'log2', None], ['gini', 'log_loss', 'entropy']]\n",
    "            },\n",
    "            \"KNClassifier\": {\n",
    "                \"class\": KNeighborsClassifier,\n",
    "                \"parameters\": [\"n_neighbors\", \"weights\", \"leaf_size\"],\n",
    "                \"values\": [[3, 5, 7, 9, 11], ['uniform', 'distance'], [10, 20, 30, 40, 50]]\n",
    "            },\n",
    "            \"SVM\": {\n",
    "                \"class\": SVC,\n",
    "                \"parameters\": [\"C\", \"kernel\", \"gamma\"],\n",
    "                \"values\": [[1, 10, 100, 1000], ['linear', 'poly', 'rbf', 'sigmoid'], ['scale', 'auto']]\n",
    "            },\n",
    "            \"AdaBoostClassifier\": {\n",
    "                \"class\": AdaBoostClassifier,\n",
    "                \"parameters\": [\"n_estimators\", \"learning_rate\"],\n",
    "                \"values\": [[10, 25, 50, 100, 150], [0.1, 0.5, 1, 1.5, 2]]\n",
    "            },\n",
    "        }\n",
    "        self.algorithms_regressor = {\n",
    "            \n",
    "            'RandomForestRegressor': {\n",
    "                'class': RandomForestRegressor,\n",
    "                'parameters': [\"n_estimators\", \"max_depth\", \"min_samples_split\", \"max_features\", \"criterion\"],\n",
    "                'values': [[10, 25, 50, 100, 150], [5, 10, 15], [2, 3, 3, 4], ['sqrt', 'log2', None], ['squared_error', 'absolute_error']]\n",
    "            },\n",
    "            'GradientBoostingRegressor': {\n",
    "                'class': GradientBoostingRegressor,\n",
    "                'parameters': [\"n_estimators\", \"learning_rate\", \"loss\"],\n",
    "                'values': [[10, 25, 50, 100, 150], [0.1, 0.5, 1, 1.5, 2], ['squared_error', 'absolute_error', 'huber']] \n",
    "            },\n",
    "            'LinearRegression': {\n",
    "                'class': LinearRegression,\n",
    "                'parameters': ['n_jobs'],\n",
    "                'values': [[3, 5, 7, 9]]\n",
    "            },\n",
    "            'LassoRegression': {\n",
    "                'class': Lasso,\n",
    "                'parameters': [\"alpha\", \"max_iter\"],\n",
    "                'values': [[0.1, 0.5, 1, 1.5, 2], [1000, 2000, 3000, 4000, 5000]]\n",
    "            },\n",
    "            'KNRegressor': {\n",
    "                'class': KNeighborsRegressor,\n",
    "                'parameters': [\"n_neighbors\", \"weights\", \"algorithm\", \"leaf_size\"],\n",
    "                'values': [[3, 5, 7, 9, 11], ['uniform', 'distance'], ['auto', 'ball_tree', 'kd_tree', 'brute'], [10, 20, 30, 40, 50]]\n",
    "            },\n",
    "        }\n",
    "        self.best_solution = None\n",
    "        self.best_score = 0\n",
    "        self.model = None\n",
    "        \n",
    "    def eval(self, model, X_train, y_train, X_val, y_val):\n",
    "        model.fit(X_train, y_train) \n",
    "        predictions = model.predict(X_val)  \n",
    "        if self.classifier:\n",
    "            score = accuracy_score(y_val, predictions) \n",
    "            #print(f'Accuracy: {score:.4f}')\n",
    "        else:\n",
    "            score = -mean_squared_error(y_val, predictions)\n",
    "            #print(f'MSE: {-score:.4f}')\n",
    "      \n",
    "        return score\n",
    "\n",
    "    def generate_neighborhood(self, current_solution):\n",
    "        algorithm_dict = self.algorithms_classifier if self.classifier else self.algorithms_regressor\n",
    "        algorithm_name = np.random.choice(list(algorithm_dict.keys()))\n",
    "        algorithm_info = algorithm_dict[algorithm_name]\n",
    "        \n",
    "        new_solution = [algorithm_name]\n",
    "        \n",
    "        if not algorithm_info['parameters']:\n",
    "            return new_solution\n",
    "        \n",
    "        while len(new_solution) < len(algorithm_info['parameters']) + 1:\n",
    "            param_index = len(new_solution) - 1\n",
    "            new_solution.append(np.random.choice(algorithm_info['values'][param_index]))\n",
    "        \n",
    "        param_idx = np.random.randint(1, len(new_solution))  # Avoid choosing the model name\n",
    "        new_solution[param_idx] = np.random.choice(algorithm_info['values'][param_idx - 1])\n",
    "        \n",
    "        if np.random.rand() < 0.1:\n",
    "            new_solution[0] = np.random.choice(list(algorithm_dict.keys()))\n",
    "            algorithm_info = algorithm_dict[new_solution[0]]\n",
    "            new_solution = [new_solution[0]] + [\n",
    "                np.random.choice(values) for values in algorithm_info[\"values\"]\n",
    "            ]\n",
    "        \n",
    "        #print(f\"Generated neighborhood for algorithm: {new_solution[0]}, parameters: {new_solution[1:]}\")\n",
    "        return new_solution\n",
    "\n",
    "\n",
    "    def create_model(self, solution):\n",
    "        algorithm_name = solution[0]\n",
    "        hyperparameters = solution[1:]\n",
    "        algorithm_dict = self.algorithms_classifier if self.classifier else self.algorithms_regressor\n",
    "        algorithm_class = algorithm_dict[algorithm_name]['class']\n",
    "            \n",
    "        if algorithm_name == 'MLPClassifier':\n",
    "            return algorithm_class(\n",
    "                max_iter=hyperparameters[0],\n",
    "                activation=hyperparameters[1],\n",
    "                solver=hyperparameters[2],\n",
    "                alpha=hyperparameters[3]\n",
    "            )\n",
    "        elif algorithm_name == 'RandomForestClassifier':\n",
    "            return algorithm_class(\n",
    "                n_estimators=hyperparameters[0],\n",
    "                max_depth=hyperparameters[1],\n",
    "                min_samples_split=hyperparameters[2],\n",
    "                max_features=hyperparameters[3],\n",
    "                criterion=hyperparameters[4]\n",
    "            )\n",
    "        elif algorithm_name == 'KNClassifier':\n",
    "            return algorithm_class(\n",
    "                n_neighbors=hyperparameters[0],\n",
    "                weights=hyperparameters[1],\n",
    "                leaf_size=hyperparameters[2]\n",
    "            )\n",
    "        elif algorithm_name == 'SVM':\n",
    "            return algorithm_class(\n",
    "                C=hyperparameters[0],\n",
    "                kernel=hyperparameters[1],\n",
    "                gamma=hyperparameters[2]\n",
    "            )\n",
    "        elif algorithm_name == 'AdaBoostClassifier':\n",
    "            return algorithm_class(\n",
    "                n_estimators=hyperparameters[0],\n",
    "                learning_rate=hyperparameters[1],\n",
    "            )\n",
    "        elif algorithm_name == 'RandomForestRegressor':\n",
    "            return algorithm_class(\n",
    "                n_estimators=hyperparameters[0],\n",
    "                max_depth=hyperparameters[1],\n",
    "                min_samples_split=hyperparameters[2],\n",
    "                max_features=hyperparameters[3],\n",
    "                criterion=hyperparameters[4]\n",
    "            )\n",
    "        elif algorithm_name == 'GradientBoostingRegressor':\n",
    "            return algorithm_class(\n",
    "                n_estimators=hyperparameters[0],\n",
    "                learning_rate=hyperparameters[1],\n",
    "                loss=hyperparameters[2]\n",
    "            )\n",
    "        elif algorithm_name == 'Polynomial Regression':\n",
    "            return algorithm_class(\n",
    "                degree=hyperparameters[0],\n",
    "                order=hyperparameters[1]\n",
    "            )\n",
    "        elif algorithm_name == 'LassoRegression':\n",
    "            return algorithm_class(\n",
    "                alpha=hyperparameters[0],\n",
    "                max_iter=hyperparameters[1]\n",
    "            )\n",
    "        elif algorithm_name == 'KNRegressor':\n",
    "            return algorithm_class(\n",
    "                n_neighbors=hyperparameters[0],\n",
    "                weights=hyperparameters[1],\n",
    "                algorithm=hyperparameters[2],\n",
    "                leaf_size=hyperparameters[3]\n",
    "            )\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val, y_val):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.simulated_annealing()\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"The model has not been fit yet. Please call the fit method first.\")\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def simulated_annealing(self):\n",
    "        start_time = time.time()  \n",
    "        # 0 rule model as initial model as base model\n",
    "        if self.classifier:\n",
    "            zero_r_model = DummyClassifier(strategy='most_frequent')\n",
    "            current_solution = ['DummyClassifier']\n",
    "        \n",
    "        else:\n",
    "            zero_r_model = DummyRegressor(strategy='mean')\n",
    "            current_solution = ['DummyRegressor']\n",
    "\n",
    "        current_score = self.eval(zero_r_model, self.X_train, self.y_train, self.X_val, self.y_val)\n",
    "        best_solution = current_solution\n",
    "        best_score = current_score\n",
    "    \n",
    "        temperature = self.initial_temp\n",
    "    \n",
    "        while time.time() - start_time < self.min_training_time:\n",
    "            for i in range(1, self.max_iterations):\n",
    "                if i % 50 == 0:\n",
    "                    print(f\"Iteration {i}, Temperature {temperature:.3f}, Best Evaluation {best_score:.5f}\")\n",
    "                    \n",
    "                if current_solution[0] == 'DummyClassifier':\n",
    "                    new_solution = self.generate_neighborhood(['KNClassifier'])\n",
    "                    new_score = self.eval(self.create_model(new_solution), self.X_train, self.y_train, self.X_val, self.y_val)\n",
    "                elif current_solution[0] == 'DummyRegressor':\n",
    "                    new_solution = self.generate_neighborhood(['KNRegressor'])\n",
    "                    new_score = self.eval(self.create_model(new_solution), self.X_train, self.y_train, self.X_val, self.y_val)\n",
    "                else:\n",
    "                    new_solution = self.generate_neighborhood(current_solution)\n",
    "                    new_score = self.eval(self.create_model(new_solution), self.X_train, self.y_train, self.X_val, self.y_val)\n",
    "    \n",
    "                if new_score > current_score or np.random.rand() < np.exp((new_score - current_score) / max(temperature, 1e-3)):\n",
    "                    current_solution = new_solution\n",
    "                    current_score = new_score\n",
    "                    if new_score > best_score:\n",
    "                        best_solution = new_solution\n",
    "                        best_score = new_score\n",
    "    \n",
    "            temperature *= self.cooling_rate\n",
    "    \n",
    "        self.best_solution = best_solution\n",
    "        self.best_score = best_score\n",
    "        self.model = self.create_model(best_solution)\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        \n",
    "        algorithm_name = best_solution[0]\n",
    "        hyperparameters = best_solution[1:]\n",
    "        if hyperparameters:\n",
    "            param_str = ', '.join(\n",
    "                f\"{param}={round(value, 4) if isinstance(value, float) else value}\"\n",
    "                for param, value in zip(self.algorithms_classifier[algorithm_name]['parameters'], hyperparameters)\n",
    "            )\n",
    "            formatted_solution = f\"{algorithm_name}({param_str})\"\n",
    "        else:\n",
    "            formatted_solution = algorithm_name  \n",
    "\n",
    "        print(f\"The best model is {formatted_solution} with a score of {round(best_score, 4)}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn Automated Machine Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TPOT Automated Machine Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Auto ML Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the AutoML algorithm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hannahknapp/Library/Python/3.10/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/hannahknapp/Library/Python/3.10/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m automl \u001b[38;5;241m=\u001b[39m AutoML_18(min_training_time\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m, max_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting the AutoML algorithm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mautoml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_iris\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_iris\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val_iris\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_iris\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluating on the test data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m predictions \u001b[38;5;241m=\u001b[39m automl\u001b[38;5;241m.\u001b[39mpredict(X_test_iris)\n",
      "Cell \u001b[0;32mIn[51], line 182\u001b[0m, in \u001b[0;36mAutoML_18.fit\u001b[0;34m(self, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_val \u001b[38;5;241m=\u001b[39m X_val\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_val \u001b[38;5;241m=\u001b[39m y_val\n\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulated_annealing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[51], line 219\u001b[0m, in \u001b[0;36mAutoML_18.simulated_annealing\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     new_solution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_neighborhood(current_solution)\n\u001b[0;32m--> 219\u001b[0m     new_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_solution\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_score \u001b[38;5;241m>\u001b[39m current_score \u001b[38;5;129;01mor\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand() \u001b[38;5;241m<\u001b[39m np\u001b[38;5;241m.\u001b[39mexp((new_score \u001b[38;5;241m-\u001b[39m current_score) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmax\u001b[39m(temperature, \u001b[38;5;241m1e-3\u001b[39m)):\n\u001b[1;32m    222\u001b[0m     current_solution \u001b[38;5;241m=\u001b[39m new_solution\n",
      "Cell \u001b[0;32mIn[51], line 71\u001b[0m, in \u001b[0;36mAutoML_18.eval\u001b[0;34m(self, model, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[1;32m     69\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_val)  \n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier:\n\u001b[0;32m---> 71\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m#print(f'Accuracy: {score:.4f}')\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mmean_squared_error(y_val, predictions)\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/sklearn/utils/_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    226\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/sklearn/metrics/_classification.py:241\u001b[0m, in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    239\u001b[0m     score \u001b[38;5;241m=\u001b[39m y_true \u001b[38;5;241m==\u001b[39m y_pred\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(\u001b[43m_average\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/sklearn/utils/_array_api.py:709\u001b[0m, in \u001b[0;36m_average\u001b[0;34m(a, axis, weights, normalize, xp)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m normalize:\n\u001b[0;32m--> 709\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(\u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    710\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    711\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(numpy\u001b[38;5;241m.\u001b[39mdot(a, weights))\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/numpy/lib/function_base.py:520\u001b[0m, in \u001b[0;36maverage\u001b[0;34m(a, axis, weights, returned, keepdims)\u001b[0m\n\u001b[1;32m    517\u001b[0m     keepdims_kw \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims}\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 520\u001b[0m     avg \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkeepdims_kw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m     avg_as_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(avg)\n\u001b[1;32m    522\u001b[0m     scl \u001b[38;5;241m=\u001b[39m avg_as_array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype(a\u001b[38;5;241m.\u001b[39msize\u001b[38;5;241m/\u001b[39mavg_as_array\u001b[38;5;241m.\u001b[39msize)\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/numpy/core/_methods.py:118\u001b[0m, in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    115\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m mu\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    116\u001b[0m         is_float16_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mumr_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, mu\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _no_nep50_warning():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "automl = AutoML_18(min_training_time=60, max_iterations=50)\n",
    "\n",
    "print(\"Fitting the AutoML algorithm\")\n",
    "automl.fit(X_train_iris, y_train_iris, X_val_iris, y_val_iris)\n",
    "\n",
    "print(\"\\nEvaluating on the test data\")\n",
    "predictions = automl.predict(X_test_iris)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test_iris, predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_iris, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our AutoML Congressional Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the AutoML algorithm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hannahknapp/Library/Python/3.10/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is MLPClassifier(max_iter=2000, activation=logistic, solver=adam, alpha=0.0001) with a score of 1.0\n",
      "\n",
      "Evaluating on the test data\n",
      "Test Accuracy: 0.9773\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      1.00      0.98        24\n",
      "         1.0       1.00      0.95      0.97        20\n",
      "\n",
      "    accuracy                           0.98        44\n",
      "   macro avg       0.98      0.97      0.98        44\n",
      "weighted avg       0.98      0.98      0.98        44\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting the AutoML algorithm\")\n",
    "automl.fit(X_train_voting, y_train_voting, X_val_voting, y_val_voting)\n",
    "\n",
    "print(\"\\nEvaluating on the test data\")\n",
    "predictions = automl.predict(X_test_voting)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test_voting, predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_voting, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our AutoML airfoil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the AutoML algorithm\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'fit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m automl \u001b[38;5;241m=\u001b[39m AutoML_18(min_training_time\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m, max_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, classifier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting the AutoML algorithm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mautoml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_airfoil\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_airfoil\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val_airfoil\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_airfoil\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluating on the test data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m predictions \u001b[38;5;241m=\u001b[39m automl\u001b[38;5;241m.\u001b[39mpredict(X_test_airfoil)\n",
      "Cell \u001b[0;32mIn[51], line 182\u001b[0m, in \u001b[0;36mAutoML_18.fit\u001b[0;34m(self, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_val \u001b[38;5;241m=\u001b[39m X_val\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_val \u001b[38;5;241m=\u001b[39m y_val\n\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulated_annealing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[51], line 219\u001b[0m, in \u001b[0;36mAutoML_18.simulated_annealing\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     new_solution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_neighborhood(current_solution)\n\u001b[0;32m--> 219\u001b[0m     new_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_solution\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_score \u001b[38;5;241m>\u001b[39m current_score \u001b[38;5;129;01mor\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand() \u001b[38;5;241m<\u001b[39m np\u001b[38;5;241m.\u001b[39mexp((new_score \u001b[38;5;241m-\u001b[39m current_score) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmax\u001b[39m(temperature, \u001b[38;5;241m1e-3\u001b[39m)):\n\u001b[1;32m    222\u001b[0m     current_solution \u001b[38;5;241m=\u001b[39m new_solution\n",
      "Cell \u001b[0;32mIn[51], line 68\u001b[0m, in \u001b[0;36mAutoML_18.eval\u001b[0;34m(self, model, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, X_train, y_train, X_val, y_val):\n\u001b[0;32m---> 68\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m(X_train, y_train) \n\u001b[1;32m     69\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_val)  \n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'fit'"
     ]
    }
   ],
   "source": [
    "automl = AutoML_18(min_training_time=60, max_iterations=10, classifier=False)\n",
    "\n",
    "print(\"Fitting the AutoML algorithm\")\n",
    "automl.fit(X_train_airfoil, y_train_airfoil, X_val_airfoil, y_val_airfoil)\n",
    "\n",
    "print(\"\\nEvaluating on the test data\")\n",
    "predictions = automl.predict(X_test_airfoil)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test_airfoil, predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_airfoil, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our AutoML abalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the AutoML algorithm\n",
      "The best model is MLPClassifier(max_iter=2000, activation=relu, solver=sgd, alpha=0.001) with a score of 0.588\n",
      "\n",
      "Evaluating on the test data\n",
      "Test Accuracy: 0.5215\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           F       0.51      0.18      0.27       278\n",
      "           I       0.66      0.76      0.71       267\n",
      "           M       0.42      0.63      0.51       291\n",
      "\n",
      "    accuracy                           0.52       836\n",
      "   macro avg       0.53      0.52      0.49       836\n",
      "weighted avg       0.53      0.52      0.49       836\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting the AutoML algorithm\")\n",
    "automl.fit(X_train_abalone, y_train_abalone, X_val_abalone, y_val_abalone)\n",
    "\n",
    "print(\"\\nEvaluating on the test data\")\n",
    "predictions = automl.predict(X_test_abalone)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test_abalone, predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_abalone, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
