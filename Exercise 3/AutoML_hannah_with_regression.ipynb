{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Machine Learning from Scratch\n",
    "\n",
    "Group 18 Members:\n",
    "\n",
    "- Clara Pichler, 11917694\n",
    "- Hannah Knapp, 11901857 \n",
    "- Sibel Toprakkiran, 09426341\n",
    "\n",
    "### Overview\n",
    "\n",
    "1. Data Set Splitting and Preprocessing\n",
    "\n",
    "2. generate neighborhood\n",
    "- `generate_neighborhood(self, current_solution)`\n",
    "\n",
    "3. create model\n",
    "- `create_model(self, solution)`\n",
    "\n",
    "4. simulated annealing\n",
    "- `simulated_annealing(self)`\n",
    "\n",
    "5. Comparison with two state of the art AutoML systems\n",
    "- auto-sklearn \n",
    "- TPOT\n",
    "\n",
    "6. Evaluation\n",
    "- Iris Dataset\n",
    "- Congressional Voting Dataset\n",
    "- gym session tracking Dataset\n",
    "- Abalone Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVC\n",
    "import time\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data sets\n",
    "\n",
    "- Iris Dataset\n",
    "- Congressional Voting Dataset\n",
    "- gym session tracking Dataset\n",
    "- Abalone Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "iris_data = pd.DataFrame(data= np.c_[iris['data'], iris['target']], columns= iris['feature_names'] + ['target'])\n",
    "iris_data['target'] = iris_data['target'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
    "\n",
    "df_voting = pd.read_csv('data/CongressionalVotingID.shuf.lrn.csv')\n",
    "\n",
    "df_airfoil = pd.read_csv(\"data/airfoil_noise_data.csv\")\n",
    "\n",
    "url='./data/abalone.csv'\n",
    "column_names = [\"Sex\", \"Length\", \"Diameter\", \"Height\", \"Whole_weight\", \"Shucked_weight\", \"Viscera_weight\", \"Shell_weight\", \"Rings\"]\n",
    "abalone_df = pd.read_csv(url, header=0, names=column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hannahknapp/Library/Python/3.10/lib/python/site-packages/sklearn/impute/_iterative.py:895: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "df_voting = df_voting.replace({\"democrat\": 0,\"republican\": 1,\"n\": 0,\"y\": 1,\"unknown\": np.nan})\n",
    "df_voting = df_voting.drop(columns=['ID'])\n",
    "\n",
    "imp = IterativeImputer(max_iter=10, random_state=0)\n",
    "df_voting = pd.DataFrame(imp.fit_transform(df_voting), columns=df_voting.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test-validation-train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_iris = iris_data.drop(['target'], axis=1)\n",
    "y_iris = iris_data['target']\n",
    "\n",
    "X_train_iris, X_temp, y_train_iris, y_temp = train_test_split(X_iris, y_iris, test_size=0.4, random_state=42)\n",
    "X_val_iris, X_test_iris, y_val_iris, y_test_iris = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_voting = df_voting.drop(['class'], axis=1)\n",
    "y_voting = df_voting['class']\n",
    "\n",
    "X_train_voting, X_temp, y_train_voting, y_temp = train_test_split(X_voting, y_voting, test_size=0.4, random_state=42)\n",
    "X_val_voting, X_test_voting, y_val_voting, y_test_voting = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_airfoil = df_airfoil.drop(['y'], axis=1)\n",
    "y_airfoil = df_airfoil['y']\n",
    "\n",
    "X_train_airfoil, X_temp, y_train_airfoil, y_temp = train_test_split(X_airfoil, y_airfoil, test_size=0.4, random_state=42)\n",
    "X_val_airfoil, X_test_airfoil, y_val_airfoil, y_test_airfoil = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_abalone = abalone_df.drop(['Sex'], axis=1)\n",
    "y_abalone = abalone_df['Sex']\n",
    "\n",
    "X_train_abalone, X_temp, y_train_abalone, y_temp = train_test_split(X_abalone, y_abalone, test_size=0.4, random_state=42)\n",
    "X_val_abalone, X_test_abalone, y_val_abalone, y_test_abalone = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "- MLP\n",
    "- RF\n",
    "- KNN \n",
    "- SVM\n",
    "- AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoML_18:\n",
    "    def __init__(self, initial_temp=100, cooling_rate=0.99, max_iterations=100, min_training_time=3600):\n",
    "        self.initial_temp = initial_temp\n",
    "        self.cooling_rate = cooling_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.min_training_time = min_training_time\n",
    "        self.classifier = True\n",
    "        self.algorithms_classifier = {\n",
    "            \"MLPClassifier\": {\n",
    "                \"class\": MLPClassifier,\n",
    "                \"parameters\": [\"max_iter\", \"activation\", \"solver\", \"alpha\"],\n",
    "                \"values\": [[1000, 2000, 3000], ['relu', 'tanh', 'logistic'], ['adam', 'sgd'], [0.0001, 0.001, 0.01]]\n",
    "            },\n",
    "            \"RandomForestClassifier\": {\n",
    "                \"class\": RandomForestClassifier,\n",
    "                \"parameters\": [\"n_estimators\", \"max_depth\", \"min_samples_split\", \"max_features\", \"criterion\"],\n",
    "                \"values\": [[10, 25, 50, 100, 150], [5, 10, 15], [2, 3, 3, 4], ['sqrt', 'log2', None], ['gini', 'log_loss', 'entropy']]\n",
    "            },\n",
    "            \"KNClassifier\": {\n",
    "                \"class\": KNeighborsClassifier,\n",
    "                \"parameters\": [\"n_neighbors\", \"weights\", \"leaf_size\"],\n",
    "                \"values\": [[3, 5, 7, 9, 11], ['uniform', 'distance'], [10, 20, 30, 40, 50]]\n",
    "            },\n",
    "            \"SVM\": {\n",
    "                \"class\": SVC,\n",
    "                \"parameters\": [\"C\", \"kernel\", \"gamma\"],\n",
    "                \"values\": [[1, 10, 100, 1000], ['linear', 'poly', 'rbf', 'sigmoid'], ['scale', 'auto']]\n",
    "            },\n",
    "            \"AdaBoostClassifier\": {\n",
    "                \"class\": AdaBoostClassifier,\n",
    "                \"parameters\": [\"n_estimators\", \"learning_rate\"],\n",
    "                \"values\": [[10, 25, 50, 100, 150], [0.1, 0.5, 1, 1.5, 2]]\n",
    "            },\n",
    "        }\n",
    "        self.algorithms_regressor = {\n",
    "            \n",
    "            'RandomForestRegressor': {\n",
    "                'class': RandomForestRegressor,\n",
    "                'parameters': [\"n_estimators\", \"max_depth\", \"min_samples_split\", \"max_features\", \"criterion\"],\n",
    "                'values': [[10, 25, 50, 100, 150], [5, 10, 15], [2, 3, 3, 4], ['sqrt', 'log2', None], ['gini', 'log_loss', 'entropy']]\n",
    "            },\n",
    "            'GradientBoostingRegressor': {\n",
    "                'class': GradientBoostingRegressor,\n",
    "                'parameters': [\"n_estimators\", \"learning_rate\", \"loss\"],\n",
    "                'values': [[10, 25, 50, 100, 150], [0.1, 0.5, 1, 1.5, 2], ['squared error', 'absolute error', 'huber']] \n",
    "            },\n",
    "            'Polynomial Regression': {\n",
    "                'class': PolynomialFeatures,\n",
    "                'parameters': [\"degree\", \"order\"],\n",
    "                'values': [[2, 3, 4, 5], ['C', 'F']]\n",
    "            },\n",
    "            'LassoRegression': {\n",
    "                'class': Lasso,\n",
    "                'parameters': [\"alpha\", \"max_iter\"],\n",
    "                'values': [[0.1, 0.5, 1, 1.5, 2], [1000, 2000, 3000, 4000, 5000]]\n",
    "            },\n",
    "            'KNRegressor': {\n",
    "                'class': KNeighborsRegressor,\n",
    "                'parameters': [\"n_neighbors\", \"weights\", \"algorithm\", \"leaf_size\"],\n",
    "                'values': [[3, 5, 7, 9, 11], ['uniform', 'distance'], ['auto', 'ball_tree', 'kd_tree', 'brute'], [10, 20, 30, 40, 50]]\n",
    "            },\n",
    "        }\n",
    "        self.best_solution = None\n",
    "        self.best_score = 0\n",
    "        self.model = None\n",
    "        \n",
    "    def eval(self, model, X_train, y_train, X_val, y_val):\n",
    "        model.fit(X_train, y_train) \n",
    "        predictions = model.predict(X_val)  \n",
    "        if self.classifier:\n",
    "            score = accuracy_score(y_val, predictions) \n",
    "            print(f'Accuracy: {score:.4f}')\n",
    "        else:\n",
    "            score = -mean_squared_error(y_val, predictions)\n",
    "            print(f'MSE: {-score:.4f}')\n",
    "      \n",
    "        return score\n",
    "\n",
    "    def generate_neighborhood(self, current_solution):\n",
    "        algorithm_dict = self.algorithms_classifier if self.classifier else self.algorithms_regressor\n",
    "        algorithm_name = np.random.choice(list(algorithm_dict.keys()))\n",
    "        algorithm_info = algorithm_dict[algorithm_name]\n",
    "        \n",
    "        new_solution = current_solution[:]\n",
    "        print(new_solution)\n",
    "        \n",
    "        if not algorithm_info['parameters']:\n",
    "            new_solution[0] = np.random.choice(list(algorithm_dict.keys()))\n",
    "            return new_solution\n",
    "    \n",
    "        while len(new_solution) < len(algorithm_info['parameters']) + 1:\n",
    "            new_solution.append(None)\n",
    "\n",
    "        param_idx = np.random.randint(1, len(new_solution))\n",
    "        new_solution[param_idx] = np.random.choice(algorithm_info['values'][param_idx - 1])\n",
    "        \n",
    "        if np.random.rand() < 0.1:\n",
    "    \n",
    "            new_solution[0] = np.random.choice(list(algorithm_dict.keys()))\n",
    "            algorithm_info = algorithm_dict[new_solution[0]]\n",
    "            \n",
    "            new_solution = [new_solution[0]] + [\n",
    "                np.random.choice(values) for values in algorithm_info[\"values\"]\n",
    "            ]\n",
    "\n",
    "        print(f\"Generated neighborhood for algorithm: {new_solution[0]}, parameters: {new_solution[1:]}\")\n",
    "        return new_solution\n",
    "\n",
    "    def create_model(self, solution):\n",
    "        algorithm_name = solution[0]\n",
    "        hyperparameters = solution[1:]\n",
    "        algorithm_dict = self.algorithms_classifier if self.classifier else self.algorithms_regressor\n",
    "        algorithm_class = algorithm_dict[algorithm_name]['class']\n",
    "            \n",
    "        if algorithm_name == 'MLPClassifier':\n",
    "            return algorithm_class(\n",
    "                max_iter=hyperparameters[0],\n",
    "                activation=hyperparameters[1],\n",
    "                solver=hyperparameters[2],\n",
    "                alpha=hyperparameters[3]\n",
    "            )\n",
    "        elif algorithm_name == 'RandomForestClassifier':\n",
    "            return algorithm_class(\n",
    "                n_estimators=hyperparameters[0],\n",
    "                max_depth=hyperparameters[1],\n",
    "                min_samples_split=hyperparameters[2],\n",
    "                max_features=hyperparameters[3],\n",
    "                criterion=hyperparameters[4]\n",
    "            )\n",
    "        elif algorithm_name == 'KNClassifier':\n",
    "            return algorithm_class(\n",
    "                n_neighbors=hyperparameters[0],\n",
    "                weights=hyperparameters[1],\n",
    "                leaf_size=hyperparameters[2]\n",
    "            )\n",
    "        elif algorithm_name == 'SVM':\n",
    "            return algorithm_class(\n",
    "                C=hyperparameters[0],\n",
    "                kernel=hyperparameters[1],\n",
    "                gamma=hyperparameters[2]\n",
    "            )\n",
    "        elif algorithm_name == 'AdaBoostClassifier':\n",
    "            return algorithm_class(\n",
    "                n_estimators=hyperparameters[0],\n",
    "                learning_rate=hyperparameters[1],\n",
    "            )\n",
    "        elif algorithm_name == 'RandomForestRegressor':\n",
    "            return algorithm_class(\n",
    "                n_estimators=hyperparameters[0],\n",
    "                max_depth=hyperparameters[1],\n",
    "                min_samples_split=hyperparameters[2],\n",
    "                max_features=hyperparameters[3],\n",
    "                criterion=hyperparameters[4]\n",
    "            )\n",
    "        elif algorithm_name == 'GradientBoostingRegressor':\n",
    "            return algorithm_class(\n",
    "                n_estimators=hyperparameters[0],\n",
    "                learning_rate=hyperparameters[1],\n",
    "                loss=hyperparameters[2]\n",
    "            )\n",
    "        elif algorithm_name == 'Polynomial Regression':\n",
    "            return algorithm_class(\n",
    "                degree=hyperparameters[0],\n",
    "                order=hyperparameters[1]\n",
    "            )\n",
    "        elif algorithm_name == 'LassoRegression':\n",
    "            return algorithm_class(\n",
    "                alpha=hyperparameters[0],\n",
    "                max_iter=hyperparameters[1]\n",
    "            )\n",
    "        elif algorithm_name == 'KNRegressor':\n",
    "            return algorithm_class(\n",
    "                n_neighbors=hyperparameters[0],\n",
    "                weights=hyperparameters[1],\n",
    "                algorithm=hyperparameters[2],\n",
    "                leaf_size=hyperparameters[3]\n",
    "            )\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val, y_val, classifier=True):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.classifier = classifier\n",
    "        self.simulated_annealing()\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"The model has not been fit yet. Please call the fit method first.\")\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def simulated_annealing(self):\n",
    "        start_time = time.time()  \n",
    "        # 0 rule model as initial model as base model\n",
    "        if self.classifier:\n",
    "            zero_r_model = DummyClassifier(strategy='most_frequent')\n",
    "            current_solution = ['DummyClassifier']\n",
    "        \n",
    "        else:\n",
    "            zero_r_model = DummyRegressor(strategy='mean')\n",
    "            current_solution = ['DummyRegressor']\n",
    "\n",
    "        current_score = self.eval(zero_r_model, self.X_train, self.y_train, self.X_val, self.y_val)\n",
    "        best_solution = current_solution\n",
    "        best_score = current_score\n",
    "    \n",
    "        temperature = self.initial_temp\n",
    "    \n",
    "        while time.time() - start_time < self.min_training_time:\n",
    "            for i in range(self.max_iterations):\n",
    "                if i % 10 == 0:\n",
    "                    print(f\"Iteration {i}, Temperature {temperature:.3f}, Best Evaluation {best_score:.5f}\")\n",
    "                    \n",
    "                if current_solution[0] == 'DummyClassifier':\n",
    "                    new_solution = self.generate_neighborhood(['RandomForestClassifier'])\n",
    "                    new_score = self.eval(self.create_model(new_solution), self.X_train, self.y_train, self.X_val, self.y_val)\n",
    "                elif current_solution[0] == 'DummyRegressor':\n",
    "                    new_solution = self.generate_neighborhood(['KNRegressor'])\n",
    "                    new_score = self.eval(self.create_model(new_solution), self.X_train, self.y_train, self.X_val, self.y_val)\n",
    "                else:\n",
    "                    new_solution = self.generate_neighborhood(current_solution)\n",
    "                    new_score = self.eval(self.create_model(new_solution), self.X_train, self.y_train, self.X_val, self.y_val)\n",
    "    \n",
    "                if new_score > current_score or np.random.rand() < np.exp((new_score - current_score) / max(temperature, 1e-3)):\n",
    "                    current_solution = new_solution\n",
    "                    current_score = new_score\n",
    "                    if new_score > best_score:\n",
    "                        best_solution = new_solution\n",
    "                        best_score = new_score\n",
    "    \n",
    "            temperature *= self.cooling_rate\n",
    "    \n",
    "        self.best_solution = best_solution\n",
    "        self.best_score = best_score\n",
    "        self.model = self.create_model(best_solution)\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        \n",
    "        algorithm_name = best_solution[0]\n",
    "        hyperparameters = best_solution[1:]\n",
    "        if hyperparameters:\n",
    "            param_str = ', '.join(\n",
    "                f\"{param}={round(value, 4) if isinstance(value, float) else value}\"\n",
    "                for param, value in zip(self.algorithms_classifier[algorithm_name]['parameters'], hyperparameters)\n",
    "            )\n",
    "            formatted_solution = f\"{algorithm_name}({param_str})\"\n",
    "        else:\n",
    "            formatted_solution = algorithm_name  \n",
    "\n",
    "        print(f\"The best model is {formatted_solution} with a score of {round(best_score, 4)}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn Automated Machine Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TPOT Automated Machine Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Auto ML Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the AutoML algorithm\n",
      "Accuracy: 0.4000\n",
      "Iteration 0, Temperature 100.000, Best Evaluation 0.40000\n",
      "['RandomForestClassifier']\n",
      "Generated neighborhood for algorithm: KNClassifier, parameters: [3, 'uniform', 10]\n",
      "Accuracy: 1.0000\n",
      "['KNClassifier', 3, 'uniform', 10]\n",
      "Generated neighborhood for algorithm: KNClassifier, parameters: [3, 'uniform', 10, None, 'log_loss']\n",
      "Accuracy: 1.0000\n",
      "['KNClassifier', 3, 'uniform', 10, None, 'log_loss']\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[134], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m automl \u001b[38;5;241m=\u001b[39m AutoML_18(min_training_time\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m, max_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting the AutoML algorithm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mautoml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_iris\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_iris\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val_iris\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_iris\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluating on the test data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m predictions \u001b[38;5;241m=\u001b[39m automl\u001b[38;5;241m.\u001b[39mpredict(X_test_iris)\n",
      "Cell \u001b[0;32mIn[115], line 185\u001b[0m, in \u001b[0;36mAutoML_18.fit\u001b[0;34m(self, X_train, y_train, X_val, y_val, classifier)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_val \u001b[38;5;241m=\u001b[39m y_val\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier \u001b[38;5;241m=\u001b[39m classifier\n\u001b[0;32m--> 185\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulated_annealing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[115], line 221\u001b[0m, in \u001b[0;36mAutoML_18.simulated_annealing\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    219\u001b[0m     new_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_model(new_solution), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_val, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_val)\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     new_solution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_neighborhood\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_solution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m     new_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_model(new_solution), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_val, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_val)\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_score \u001b[38;5;241m>\u001b[39m current_score \u001b[38;5;129;01mor\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand() \u001b[38;5;241m<\u001b[39m np\u001b[38;5;241m.\u001b[39mexp((new_score \u001b[38;5;241m-\u001b[39m current_score) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmax\u001b[39m(temperature, \u001b[38;5;241m1e-3\u001b[39m)):\n",
      "Cell \u001b[0;32mIn[115], line 95\u001b[0m, in \u001b[0;36mAutoML_18.generate_neighborhood\u001b[0;34m(self, current_solution)\u001b[0m\n\u001b[1;32m     92\u001b[0m     new_solution\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     94\u001b[0m param_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(new_solution))\n\u001b[0;32m---> 95\u001b[0m new_solution[param_idx] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[43malgorithm_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mparam_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.1\u001b[39m:\n\u001b[1;32m     99\u001b[0m     new_solution[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mlist\u001b[39m(algorithm_dict\u001b[38;5;241m.\u001b[39mkeys()))\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "automl = AutoML_18(min_training_time=60, max_iterations=10)\n",
    "\n",
    "print(\"Fitting the AutoML algorithm\")\n",
    "automl.fit(X_train_iris, y_train_iris, X_val_iris, y_val_iris)\n",
    "\n",
    "print(\"\\nEvaluating on the test data\")\n",
    "predictions = automl.predict(X_test_iris)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test_iris, predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_iris, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our AutoML Congressional Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the AutoML algorithm\n",
      "MSE: 0.2581\n",
      "Iteration 0, Temperature 100.000, Best Evaluation -0.25807\n",
      "{'RandomForestRegressor': {'class': <class 'sklearn.ensemble._forest.RandomForestRegressor'>, 'parameters': ['n_estimators', 'max_depth', 'min_samples_split', 'max_features', 'criterion'], 'values': [[10, 25, 50, 100, 150], [5, 10, 15], [2, 3, 3, 4], ['sqrt', 'log2', None], ['gini', 'log_loss', 'entropy']]}, 'GradientBoostingRegressor': {'class': <class 'sklearn.ensemble._gb.GradientBoostingRegressor'>, 'parameters': ['n_estimators', 'learning_rate', 'loss'], 'values': [[10, 25, 50, 100, 150], [0.1, 0.5, 1, 1.5, 2], ['squared error', 'absolute error', 'huber']]}, 'LinearRegression': {'class': <class 'sklearn.linear_model._base.LinearRegression'>, 'parameters': [], 'values': []}, 'LassoRegression': {'class': <class 'sklearn.linear_model._coordinate_descent.Lasso'>, 'parameters': ['alpha', 'max_iter'], 'values': [[0.1, 0.5, 1, 1.5, 2], [1000, 2000, 3000, 4000, 5000]]}, 'KNRegressor': {'class': <class 'sklearn.neighbors._regression.KNeighborsRegressor'>, 'parameters': ['n_neighbors', 'weights', 'algorithm', 'leaf_size'], 'values': [[3, 5, 7, 9, 11], ['uniform', 'distance'], ['auto', 'ball_tree', 'kd_tree', 'brute'], [10, 20, 30, 40, 50]]}}\n",
      "[]\n",
      "<class 'sklearn.linear_model._base.LinearRegression'>\n",
      "MSE: 0.0455\n",
      "{'RandomForestRegressor': {'class': <class 'sklearn.ensemble._forest.RandomForestRegressor'>, 'parameters': ['n_estimators', 'max_depth', 'min_samples_split', 'max_features', 'criterion'], 'values': [[10, 25, 50, 100, 150], [5, 10, 15], [2, 3, 3, 4], ['sqrt', 'log2', None], ['gini', 'log_loss', 'entropy']]}, 'GradientBoostingRegressor': {'class': <class 'sklearn.ensemble._gb.GradientBoostingRegressor'>, 'parameters': ['n_estimators', 'learning_rate', 'loss'], 'values': [[10, 25, 50, 100, 150], [0.1, 0.5, 1, 1.5, 2], ['squared error', 'absolute error', 'huber']]}, 'LinearRegression': {'class': <class 'sklearn.linear_model._base.LinearRegression'>, 'parameters': [], 'values': []}, 'LassoRegression': {'class': <class 'sklearn.linear_model._coordinate_descent.Lasso'>, 'parameters': ['alpha', 'max_iter'], 'values': [[0.1, 0.5, 1, 1.5, 2], [1000, 2000, 3000, 4000, 5000]]}, 'KNRegressor': {'class': <class 'sklearn.neighbors._regression.KNeighborsRegressor'>, 'parameters': ['n_neighbors', 'weights', 'algorithm', 'leaf_size'], 'values': [[3, 5, 7, 9, 11], ['uniform', 'distance'], ['auto', 'ball_tree', 'kd_tree', 'brute'], [10, 20, 30, 40, 50]]}}\n",
      "['n_neighbors', 'weights', 'algorithm', 'leaf_size']\n",
      "Generated neighborhood for algorithm: LinearRegression, parameters: [7, 'distance', 'brute', 30]\n",
      "<class 'sklearn.linear_model._base.LinearRegression'>\n",
      "MSE: 0.0455\n",
      "{'RandomForestRegressor': {'class': <class 'sklearn.ensemble._forest.RandomForestRegressor'>, 'parameters': ['n_estimators', 'max_depth', 'min_samples_split', 'max_features', 'criterion'], 'values': [[10, 25, 50, 100, 150], [5, 10, 15], [2, 3, 3, 4], ['sqrt', 'log2', None], ['gini', 'log_loss', 'entropy']]}, 'GradientBoostingRegressor': {'class': <class 'sklearn.ensemble._gb.GradientBoostingRegressor'>, 'parameters': ['n_estimators', 'learning_rate', 'loss'], 'values': [[10, 25, 50, 100, 150], [0.1, 0.5, 1, 1.5, 2], ['squared error', 'absolute error', 'huber']]}, 'LinearRegression': {'class': <class 'sklearn.linear_model._base.LinearRegression'>, 'parameters': [], 'values': []}, 'LassoRegression': {'class': <class 'sklearn.linear_model._coordinate_descent.Lasso'>, 'parameters': ['alpha', 'max_iter'], 'values': [[0.1, 0.5, 1, 1.5, 2], [1000, 2000, 3000, 4000, 5000]]}, 'KNRegressor': {'class': <class 'sklearn.neighbors._regression.KNeighborsRegressor'>, 'parameters': ['n_neighbors', 'weights', 'algorithm', 'leaf_size'], 'values': [[3, 5, 7, 9, 11], ['uniform', 'distance'], ['auto', 'ball_tree', 'kd_tree', 'brute'], [10, 20, 30, 40, 50]]}}\n",
      "['n_neighbors', 'weights', 'algorithm', 'leaf_size']\n",
      "Generated neighborhood for algorithm: LinearRegression, parameters: [7, 'distance', 'brute', 30]\n",
      "<class 'sklearn.linear_model._base.LinearRegression'>\n",
      "MSE: 0.0455\n",
      "{'RandomForestRegressor': {'class': <class 'sklearn.ensemble._forest.RandomForestRegressor'>, 'parameters': ['n_estimators', 'max_depth', 'min_samples_split', 'max_features', 'criterion'], 'values': [[10, 25, 50, 100, 150], [5, 10, 15], [2, 3, 3, 4], ['sqrt', 'log2', None], ['gini', 'log_loss', 'entropy']]}, 'GradientBoostingRegressor': {'class': <class 'sklearn.ensemble._gb.GradientBoostingRegressor'>, 'parameters': ['n_estimators', 'learning_rate', 'loss'], 'values': [[10, 25, 50, 100, 150], [0.1, 0.5, 1, 1.5, 2], ['squared error', 'absolute error', 'huber']]}, 'LinearRegression': {'class': <class 'sklearn.linear_model._base.LinearRegression'>, 'parameters': [], 'values': []}, 'LassoRegression': {'class': <class 'sklearn.linear_model._coordinate_descent.Lasso'>, 'parameters': ['alpha', 'max_iter'], 'values': [[0.1, 0.5, 1, 1.5, 2], [1000, 2000, 3000, 4000, 5000]]}, 'KNRegressor': {'class': <class 'sklearn.neighbors._regression.KNeighborsRegressor'>, 'parameters': ['n_neighbors', 'weights', 'algorithm', 'leaf_size'], 'values': [[3, 5, 7, 9, 11], ['uniform', 'distance'], ['auto', 'ball_tree', 'kd_tree', 'brute'], [10, 20, 30, 40, 50]]}}\n",
      "['n_estimators', 'max_depth', 'min_samples_split', 'max_features', 'criterion']\n",
      "Generated neighborhood for algorithm: LinearRegression, parameters: [7, 'distance', 'brute', 'log2', 'log_loss']\n",
      "<class 'sklearn.linear_model._base.LinearRegression'>\n",
      "MSE: 0.0455\n",
      "{'RandomForestRegressor': {'class': <class 'sklearn.ensemble._forest.RandomForestRegressor'>, 'parameters': ['n_estimators', 'max_depth', 'min_samples_split', 'max_features', 'criterion'], 'values': [[10, 25, 50, 100, 150], [5, 10, 15], [2, 3, 3, 4], ['sqrt', 'log2', None], ['gini', 'log_loss', 'entropy']]}, 'GradientBoostingRegressor': {'class': <class 'sklearn.ensemble._gb.GradientBoostingRegressor'>, 'parameters': ['n_estimators', 'learning_rate', 'loss'], 'values': [[10, 25, 50, 100, 150], [0.1, 0.5, 1, 1.5, 2], ['squared error', 'absolute error', 'huber']]}, 'LinearRegression': {'class': <class 'sklearn.linear_model._base.LinearRegression'>, 'parameters': [], 'values': []}, 'LassoRegression': {'class': <class 'sklearn.linear_model._coordinate_descent.Lasso'>, 'parameters': ['alpha', 'max_iter'], 'values': [[0.1, 0.5, 1, 1.5, 2], [1000, 2000, 3000, 4000, 5000]]}, 'KNRegressor': {'class': <class 'sklearn.neighbors._regression.KNeighborsRegressor'>, 'parameters': ['n_neighbors', 'weights', 'algorithm', 'leaf_size'], 'values': [[3, 5, 7, 9, 11], ['uniform', 'distance'], ['auto', 'ball_tree', 'kd_tree', 'brute'], [10, 20, 30, 40, 50]]}}\n",
      "['n_estimators', 'learning_rate', 'loss']\n",
      "Generated neighborhood for algorithm: LinearRegression, parameters: [7, 'distance', 'absolute error', 'log2', 'log_loss']\n",
      "<class 'sklearn.linear_model._base.LinearRegression'>\n",
      "MSE: 0.0455\n",
      "{'RandomForestRegressor': {'class': <class 'sklearn.ensemble._forest.RandomForestRegressor'>, 'parameters': ['n_estimators', 'max_depth', 'min_samples_split', 'max_features', 'criterion'], 'values': [[10, 25, 50, 100, 150], [5, 10, 15], [2, 3, 3, 4], ['sqrt', 'log2', None], ['gini', 'log_loss', 'entropy']]}, 'GradientBoostingRegressor': {'class': <class 'sklearn.ensemble._gb.GradientBoostingRegressor'>, 'parameters': ['n_estimators', 'learning_rate', 'loss'], 'values': [[10, 25, 50, 100, 150], [0.1, 0.5, 1, 1.5, 2], ['squared error', 'absolute error', 'huber']]}, 'LinearRegression': {'class': <class 'sklearn.linear_model._base.LinearRegression'>, 'parameters': [], 'values': []}, 'LassoRegression': {'class': <class 'sklearn.linear_model._coordinate_descent.Lasso'>, 'parameters': ['alpha', 'max_iter'], 'values': [[0.1, 0.5, 1, 1.5, 2], [1000, 2000, 3000, 4000, 5000]]}, 'KNRegressor': {'class': <class 'sklearn.neighbors._regression.KNeighborsRegressor'>, 'parameters': ['n_neighbors', 'weights', 'algorithm', 'leaf_size'], 'values': [[3, 5, 7, 9, 11], ['uniform', 'distance'], ['auto', 'ball_tree', 'kd_tree', 'brute'], [10, 20, 30, 40, 50]]}}\n",
      "['alpha', 'max_iter']\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting the AutoML algorithm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mautoml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_voting\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_voting\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val_voting\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_voting\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluating on the test data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m predictions \u001b[38;5;241m=\u001b[39m automl\u001b[38;5;241m.\u001b[39mpredict(X_test_voting)\n",
      "Cell \u001b[0;32mIn[66], line 191\u001b[0m, in \u001b[0;36mAutoML_18.fit\u001b[0;34m(self, X_train, y_train, X_val, y_val, classifier)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_val \u001b[38;5;241m=\u001b[39m y_val\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier \u001b[38;5;241m=\u001b[39m classifier\n\u001b[0;32m--> 191\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulated_annealing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[66], line 227\u001b[0m, in \u001b[0;36mAutoML_18.simulated_annealing\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    225\u001b[0m     new_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_model(new_solution), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_val, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_val)\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 227\u001b[0m     new_solution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_neighborhood\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_solution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     new_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_model(new_solution), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_val, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_val)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_score \u001b[38;5;241m>\u001b[39m current_score \u001b[38;5;129;01mor\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand() \u001b[38;5;241m<\u001b[39m np\u001b[38;5;241m.\u001b[39mexp((new_score \u001b[38;5;241m-\u001b[39m current_score) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmax\u001b[39m(temperature, \u001b[38;5;241m1e-3\u001b[39m)):\n",
      "Cell \u001b[0;32mIn[66], line 103\u001b[0m, in \u001b[0;36mAutoML_18.generate_neighborhood\u001b[0;34m(self, current_solution)\u001b[0m\n\u001b[1;32m     99\u001b[0m     new_solution\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(algorithm_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m'\u001b[39m][param_index]))\n\u001b[1;32m    102\u001b[0m param_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(new_solution))\n\u001b[0;32m--> 103\u001b[0m new_solution[param_idx] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[43malgorithm_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mparam_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.1\u001b[39m:\n\u001b[1;32m    107\u001b[0m     new_solution[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mlist\u001b[39m(algorithm_dict\u001b[38;5;241m.\u001b[39mkeys()))\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(\"Fitting the AutoML algorithm\")\n",
    "automl.fit(X_train_voting, y_train_voting, X_val_voting, y_val_voting, classifier=False)\n",
    "\n",
    "print(\"\\nEvaluating on the test data\")\n",
    "predictions = automl.predict(X_test_voting)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test_voting, predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_voting, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our AutoML airfoil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the AutoML algorithm\n",
      "MSE: 45.4368\n",
      "Iteration 0, Temperature 100.000, Best Evaluation -45.43682\n",
      "Generated neighborhood for algorithm: KNRegressor, parameters: [0.5, 3000]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m automl \u001b[38;5;241m=\u001b[39m AutoML_18(min_training_time\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m, max_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting the AutoML algorithm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mautoml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_airfoil\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_airfoil\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val_airfoil\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_airfoil\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluating on the test data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m predictions \u001b[38;5;241m=\u001b[39m automl\u001b[38;5;241m.\u001b[39mpredict(X_test_airfoil)\n",
      "Cell \u001b[0;32mIn[50], line 189\u001b[0m, in \u001b[0;36mAutoML_18.fit\u001b[0;34m(self, X_train, y_train, X_val, y_val, classifier)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_val \u001b[38;5;241m=\u001b[39m y_val\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier \u001b[38;5;241m=\u001b[39m classifier\n\u001b[0;32m--> 189\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulated_annealing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[50], line 223\u001b[0m, in \u001b[0;36mAutoML_18.simulated_annealing\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m current_solution[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDummyRegressor\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    222\u001b[0m     new_solution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_neighborhood([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKNRegressor\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 223\u001b[0m     new_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_solution\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_val, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_val)\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m     new_solution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_neighborhood(current_solution)\n",
      "Cell \u001b[0;32mIn[50], line 179\u001b[0m, in \u001b[0;36mAutoML_18.create_model\u001b[0;34m(self, solution)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m algorithm_class(\n\u001b[1;32m    172\u001b[0m         alpha\u001b[38;5;241m=\u001b[39mhyperparameters[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    173\u001b[0m         max_iter\u001b[38;5;241m=\u001b[39mhyperparameters[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    174\u001b[0m     )\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m algorithm_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKNRegressor\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m algorithm_class(\n\u001b[1;32m    177\u001b[0m         n_neighbors\u001b[38;5;241m=\u001b[39mhyperparameters[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    178\u001b[0m         weights\u001b[38;5;241m=\u001b[39mhyperparameters[\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m--> 179\u001b[0m         algorithm\u001b[38;5;241m=\u001b[39m\u001b[43mhyperparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m    180\u001b[0m         leaf_size\u001b[38;5;241m=\u001b[39mhyperparameters[\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m    181\u001b[0m     )\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "automl = AutoML_18(min_training_time=60, max_iterations=10)\n",
    "\n",
    "print(\"Fitting the AutoML algorithm\")\n",
    "automl.fit(X_train_airfoil, y_train_airfoil, X_val_airfoil, y_val_airfoil, classifier=False)\n",
    "\n",
    "print(\"\\nEvaluating on the test data\")\n",
    "predictions = automl.predict(X_test_airfoil)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test_airfoil, predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_airfoil, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our AutoML abalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the AutoML algorithm\n",
      "Initial model: DummyClassifier\n",
      "Initial parameters: (strategy='most_frequent')\n",
      "Accuracy: 0.3605\n",
      "Iteration 0, Temperature 100.000, Best Evaluation 0.36048\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [5, 2, 'sqrt', 'gini']\n",
      "Accuracy: 0.5461\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [50, 2.0]\n",
      "Accuracy: 0.5138\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [50, 1.0]\n",
      "Accuracy: 0.5425\n",
      "Generated neighborhood for algorithm: RF, parameters: [150, 15, 3, None, 'gini']\n",
      "Accuracy: 0.5317\n",
      "Generated neighborhood for algorithm: RF, parameters: [150, 15, 3, None, 'entropy']\n",
      "Accuracy: 0.5210\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [15, 2, None, 'log_loss']\n",
      "Accuracy: 0.4898\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [15, 2, None, 'log_loss']\n",
      "Accuracy: 0.4874\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [15, 3, None, 'log_loss']\n",
      "Accuracy: 0.5006\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [15, 3, 'log2', 'log_loss']\n",
      "Accuracy: 0.5126\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [15, 2, 'log2', 'log_loss']\n",
      "Accuracy: 0.4910\n",
      "Iteration 10, Temperature 100.000, Best Evaluation 0.54611\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [15, 2, 'log2', 'gini']\n",
      "Accuracy: 0.5090\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [15, 3, 'log2', 'gini']\n",
      "Accuracy: 0.5054\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [15, 3, 'log2', 'gini']\n",
      "Accuracy: 0.5054\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [15, 4, 'log2', 'gini']\n",
      "Accuracy: 0.4994\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [10, 4, 'log2', 'gini']\n",
      "Accuracy: 0.4838\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [15, 4, 'log2', 'gini']\n",
      "Accuracy: 0.5102\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [15, 4, 'log2', 'gini']\n",
      "Accuracy: 0.4898\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [15, 4, 'log2', 'log_loss']\n",
      "Accuracy: 0.4802\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [10, 4, 'log2', 'log_loss']\n",
      "Accuracy: 0.5210\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [10, 3, 'log2', 'log_loss']\n",
      "Accuracy: 0.5293\n",
      "Iteration 20, Temperature 100.000, Best Evaluation 0.54611\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [10, 3, 'log2', 'log_loss']\n",
      "Accuracy: 0.5317\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [10, 4, 'log2', 'log_loss']\n",
      "Accuracy: 0.5126\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [10, 4, 'log2', 'entropy']\n",
      "Accuracy: 0.5030\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [5, 4, 'log2', 'entropy']\n",
      "Accuracy: 0.5449\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [15, 4, 'log2', 'entropy']\n",
      "Accuracy: 0.4994\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [15, 4, 'log2', 'entropy']\n",
      "Accuracy: 0.5257\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [15, 3, 'log2', 'entropy']\n",
      "Accuracy: 0.5054\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [15, 3, 'log2', 'entropy']\n",
      "Accuracy: 0.5162\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [50, 2.0]\n",
      "Accuracy: 0.5138\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [25, 2.0]\n",
      "Accuracy: 0.5078\n",
      "Iteration 30, Temperature 100.000, Best Evaluation 0.54611\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [25, 0.1]\n",
      "Accuracy: 0.5473\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [25, 0.1]\n",
      "Accuracy: 0.5473\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [5, 3, None, 'log_loss']\n",
      "Accuracy: 0.5341\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [5, 3, None, 'entropy']\n",
      "Accuracy: 0.5341\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [5, 3, None, 'entropy']\n",
      "Accuracy: 0.5353\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [5, 3, None, 'entropy']\n",
      "Accuracy: 0.5365\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [5, 3, None, 'gini']\n",
      "Accuracy: 0.5473\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [5, 3, None, 'gini']\n",
      "Accuracy: 0.5461\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [5, 3, None, 'gini']\n",
      "Accuracy: 0.5461\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [5, 3, 'log2', 'gini']\n",
      "Accuracy: 0.5593\n",
      "Iteration 40, Temperature 100.000, Best Evaluation 0.55928\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [5, 3, 'log2', 'log_loss']\n",
      "Accuracy: 0.5497\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [5, 3, 'log2', 'log_loss']\n",
      "Accuracy: 0.5353\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [10, 3, 'log2', 'log_loss']\n",
      "Accuracy: 0.5461\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [10, 3, 'log2', 'log_loss']\n",
      "Accuracy: 0.5341\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [10, 2, 'log2', 'log_loss']\n",
      "Accuracy: 0.5389\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [10, 3, 'log2', 'log_loss']\n",
      "Accuracy: 0.5090\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [15, 3, 'log2', 'log_loss']\n",
      "Accuracy: 0.5054\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [15, 3, 'log2', 'log_loss']\n",
      "Accuracy: 0.5162\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [15, 3, 'log2', 'entropy']\n",
      "Accuracy: 0.4838\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [15, 3, 'log2', 'entropy']\n",
      "Accuracy: 0.5138\n",
      "Iteration 50, Temperature 100.000, Best Evaluation 0.55928\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [15, 3, None, 'entropy']\n",
      "Accuracy: 0.4934\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [15, 3, None, 'entropy']\n",
      "Accuracy: 0.5018\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [15, 3, None, 'log_loss']\n",
      "Accuracy: 0.4874\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [15, 3, 'sqrt', 'log_loss']\n",
      "Accuracy: 0.5078\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [15, 3, 'log2', 'log_loss']\n",
      "Accuracy: 0.4922\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [15, 3, 'log2', 'log_loss']\n",
      "Accuracy: 0.4898\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [15, 3, 'log2', 'entropy']\n",
      "Accuracy: 0.5198\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [15, 3, 'log2', 'gini']\n",
      "Accuracy: 0.4970\n",
      "Generated neighborhood for algorithm: KNN, parameters: [7, 'distance', 'brute', 40]\n",
      "Accuracy: 0.5473\n",
      "Generated neighborhood for algorithm: KNN, parameters: [7, 'distance', 'kd_tree', 40]\n",
      "Accuracy: 0.5473\n",
      "Iteration 60, Temperature 100.000, Best Evaluation 0.55928\n",
      "Generated neighborhood for algorithm: KNN, parameters: [7, 'distance', 'ball_tree', 40]\n",
      "Accuracy: 0.5473\n",
      "Generated neighborhood for algorithm: KNN, parameters: [7, 'distance', 'brute', 40]\n",
      "Accuracy: 0.5473\n",
      "Generated neighborhood for algorithm: KNN, parameters: [7, 'distance', 'brute', 40]\n",
      "Accuracy: 0.5473\n",
      "Generated neighborhood for algorithm: KNN, parameters: [7, 'distance', 'ball_tree', 40]\n",
      "Accuracy: 0.5473\n",
      "Generated neighborhood for algorithm: KNN, parameters: [7, 'distance', 'ball_tree', 30]\n",
      "Accuracy: 0.5473\n",
      "Generated neighborhood for algorithm: KNN, parameters: [7, 'distance', 'ball_tree', 30]\n",
      "Accuracy: 0.5473\n",
      "Generated neighborhood for algorithm: KNN, parameters: [7, 'distance', 'ball_tree', 30]\n",
      "Accuracy: 0.5473\n",
      "Generated neighborhood for algorithm: KNN, parameters: [7, 'distance', 'ball_tree', 20]\n",
      "Accuracy: 0.5473\n",
      "Generated neighborhood for algorithm: KNN, parameters: [7, 'distance', 'auto', 20]\n",
      "Accuracy: 0.5473\n",
      "Generated neighborhood for algorithm: KNN, parameters: [7, 'distance', 'auto', 20]\n",
      "Accuracy: 0.5473\n",
      "Iteration 70, Temperature 100.000, Best Evaluation 0.55928\n",
      "Generated neighborhood for algorithm: KNN, parameters: [7, 'distance', 'auto', 20]\n",
      "Accuracy: 0.5473\n",
      "Generated neighborhood for algorithm: KNN, parameters: [7, 'distance', 'auto', 20]\n",
      "Accuracy: 0.5473\n",
      "Generated neighborhood for algorithm: KNN, parameters: [7, 'uniform', 'auto', 20]\n",
      "Accuracy: 0.5413\n",
      "Generated neighborhood for algorithm: KNN, parameters: [7, 'uniform', 'auto', 20]\n",
      "Accuracy: 0.5413\n",
      "Generated neighborhood for algorithm: KNN, parameters: [7, 'uniform', 'auto', 50]\n",
      "Accuracy: 0.5413\n",
      "Generated neighborhood for algorithm: KNN, parameters: [7, 'uniform', 'kd_tree', 50]\n",
      "Accuracy: 0.5413\n",
      "Generated neighborhood for algorithm: KNN, parameters: [5, 'uniform', 'kd_tree', 50]\n",
      "Accuracy: 0.5317\n",
      "Generated neighborhood for algorithm: KNN, parameters: [5, 'uniform', 'kd_tree', 40]\n",
      "Accuracy: 0.5317\n",
      "Generated neighborhood for algorithm: KNN, parameters: [7, 'uniform', 'kd_tree', 40]\n",
      "Accuracy: 0.5413\n",
      "Generated neighborhood for algorithm: KNN, parameters: [9, 'uniform', 'kd_tree', 40]\n",
      "Accuracy: 0.5389\n",
      "Iteration 80, Temperature 100.000, Best Evaluation 0.55928\n",
      "Generated neighborhood for algorithm: KNN, parameters: [9, 'uniform', 'kd_tree', 40]\n",
      "Accuracy: 0.5389\n",
      "Generated neighborhood for algorithm: KNN, parameters: [9, 'distance', 'kd_tree', 40]\n",
      "Accuracy: 0.5437\n",
      "Generated neighborhood for algorithm: KNN, parameters: [9, 'distance', 'auto', 40]\n",
      "Accuracy: 0.5437\n",
      "Generated neighborhood for algorithm: KNN, parameters: [9, 'distance', 'ball_tree', 40]\n",
      "Accuracy: 0.5437\n",
      "Generated neighborhood for algorithm: KNN, parameters: [9, 'distance', 'brute', 40]\n",
      "Accuracy: 0.5437\n",
      "Generated neighborhood for algorithm: KNN, parameters: [9, 'uniform', 'brute', 40]\n",
      "Accuracy: 0.5389\n",
      "Generated neighborhood for algorithm: KNN, parameters: [7, 'uniform', 'brute', 40]\n",
      "Accuracy: 0.5413\n",
      "Generated neighborhood for algorithm: KNN, parameters: [7, 'uniform', 'brute', 40]\n",
      "Accuracy: 0.5413\n",
      "Generated neighborhood for algorithm: KNN, parameters: [7, 'uniform', 'brute', 40]\n",
      "Accuracy: 0.5413\n",
      "Generated neighborhood for algorithm: KNN, parameters: [7, 'distance', 'brute', 40]\n",
      "Accuracy: 0.5473\n",
      "Iteration 90, Temperature 100.000, Best Evaluation 0.55928\n",
      "Generated neighborhood for algorithm: KNN, parameters: [7, 'distance', 'ball_tree', 40]\n",
      "Accuracy: 0.5473\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [25, 0.5]\n",
      "Accuracy: 0.5569\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [25, 0.5]\n",
      "Accuracy: 0.5569\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [50, 0.5]\n",
      "Accuracy: 0.5629\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [50, 1.5]\n",
      "Accuracy: 0.5257\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [50, 1.5]\n",
      "Accuracy: 0.5257\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [50, 2.0]\n",
      "Accuracy: 0.5138\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [50, 2.0]\n",
      "Accuracy: 0.5138\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [50, 1.5]\n",
      "Accuracy: 0.5257\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [50, 0.1]\n",
      "Accuracy: 0.5473\n",
      "Iteration 0, Temperature 99.000, Best Evaluation 0.56287\n",
      "Generated neighborhood for algorithm: MLP, parameters: [3000, 'relu', 'adam', 0.0001]\n",
      "Accuracy: 0.5749\n",
      "Generated neighborhood for algorithm: MLP, parameters: [1000, 'relu', 'adam', 0.0001]\n",
      "Accuracy: 0.5653\n",
      "Generated neighborhood for algorithm: MLP, parameters: [1000, 'relu', 'sgd', 0.0001]\n",
      "Accuracy: 0.5832\n",
      "Generated neighborhood for algorithm: MLP, parameters: [2000, 'relu', 'sgd', 0.0001]\n",
      "Accuracy: 0.5808\n",
      "Generated neighborhood for algorithm: MLP, parameters: [3000, 'relu', 'sgd', 0.0001]\n",
      "Accuracy: 0.5916\n",
      "Generated neighborhood for algorithm: MLP, parameters: [3000, 'relu', 'sgd', 0.001]\n",
      "Accuracy: 0.5832\n",
      "Generated neighborhood for algorithm: MLP, parameters: [2000, 'relu', 'sgd', 0.001]\n",
      "Accuracy: 0.5808\n",
      "Generated neighborhood for algorithm: MLP, parameters: [2000, 'relu', 'sgd', 0.0001]\n",
      "Accuracy: 0.5760\n",
      "Generated neighborhood for algorithm: MLP, parameters: [2000, 'relu', 'sgd', 0.0001]\n",
      "Accuracy: 0.5784\n",
      "Generated neighborhood for algorithm: MLP, parameters: [2000, 'logistic', 'sgd', 0.0001]\n",
      "Accuracy: 0.5725\n",
      "Iteration 10, Temperature 99.000, Best Evaluation 0.59162\n",
      "Generated neighborhood for algorithm: RF, parameters: [25, 5, 3, 'sqrt', 'log_loss']\n",
      "Accuracy: 0.5605\n",
      "Generated neighborhood for algorithm: RF, parameters: [10, 5, 3, 'sqrt', 'log_loss']\n",
      "Accuracy: 0.5701\n",
      "Generated neighborhood for algorithm: RF, parameters: [10, 15, 3, 'sqrt', 'log_loss']\n",
      "Accuracy: 0.5317\n",
      "Generated neighborhood for algorithm: RF, parameters: [10, 15, 3, 'sqrt', 'log_loss']\n",
      "Accuracy: 0.5389\n",
      "Generated neighborhood for algorithm: RF, parameters: [10, 15, 3, 'sqrt', 'log_loss']\n",
      "Accuracy: 0.5449\n",
      "Generated neighborhood for algorithm: RF, parameters: [50, 15, 3, 'sqrt', 'log_loss']\n",
      "Accuracy: 0.5437\n",
      "Generated neighborhood for algorithm: RF, parameters: [50, 15, 3, 'sqrt', 'gini']\n",
      "Accuracy: 0.5353\n",
      "Generated neighborhood for algorithm: RF, parameters: [10, 15, 3, 'sqrt', 'gini']\n",
      "Accuracy: 0.5281\n",
      "Generated neighborhood for algorithm: RF, parameters: [25, 15, 3, 'sqrt', 'gini']\n",
      "Accuracy: 0.5473\n",
      "Generated neighborhood for algorithm: RF, parameters: [25, 15, 3, 'sqrt', 'gini']\n",
      "Accuracy: 0.5377\n",
      "Iteration 20, Temperature 99.000, Best Evaluation 0.59162\n",
      "Generated neighborhood for algorithm: RF, parameters: [25, 15, 2, 'sqrt', 'gini']\n",
      "Accuracy: 0.5449\n",
      "Generated neighborhood for algorithm: RF, parameters: [25, 15, 3, 'sqrt', 'gini']\n",
      "Accuracy: 0.5485\n",
      "Generated neighborhood for algorithm: MLP, parameters: [3000, 'relu', 'adam', 0.0001]\n",
      "Accuracy: 0.5856\n",
      "Generated neighborhood for algorithm: MLP, parameters: [3000, 'relu', 'adam', 0.01]\n",
      "Accuracy: 0.5737\n",
      "Generated neighborhood for algorithm: MLP, parameters: [3000, 'relu', 'adam', 0.0001]\n",
      "Accuracy: 0.5677\n",
      "Generated neighborhood for algorithm: MLP, parameters: [3000, 'relu', 'adam', 0.001]\n",
      "Accuracy: 0.5473\n",
      "Generated neighborhood for algorithm: MLP, parameters: [3000, 'relu', 'adam', 0.01]\n",
      "Accuracy: 0.5665\n",
      "Generated neighborhood for algorithm: MLP, parameters: [3000, 'tanh', 'adam', 0.01]\n",
      "Accuracy: 0.5234\n",
      "Generated neighborhood for algorithm: MLP, parameters: [1000, 'tanh', 'adam', 0.01]\n",
      "Accuracy: 0.5880\n",
      "Generated neighborhood for algorithm: MLP, parameters: [3000, 'tanh', 'adam', 0.01]\n",
      "Accuracy: 0.5593\n",
      "Iteration 30, Temperature 99.000, Best Evaluation 0.59162\n",
      "Generated neighborhood for algorithm: MLP, parameters: [2000, 'tanh', 'adam', 0.01]\n",
      "Accuracy: 0.5760\n",
      "Generated neighborhood for algorithm: MLP, parameters: [2000, 'tanh', 'adam', 0.01]\n",
      "Accuracy: 0.5832\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [10, 4, 'log2', 'log_loss']\n",
      "Accuracy: 0.4958\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [10, 4, 'log2', 'entropy']\n",
      "Accuracy: 0.5234\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [10, 4, 'log2', 'entropy']\n",
      "Accuracy: 0.5114\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [10, 4, None, 'entropy']\n",
      "Accuracy: 0.4958\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [5, 4, None, 'entropy']\n",
      "Accuracy: 0.5365\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [15, 4, None, 'entropy']\n",
      "Accuracy: 0.4946\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [5, 4, None, 'entropy']\n",
      "Accuracy: 0.5353\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [5, 3, None, 'entropy']\n",
      "Accuracy: 0.5365\n",
      "Iteration 40, Temperature 99.000, Best Evaluation 0.59162\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [5, 3, None, 'gini']\n",
      "Accuracy: 0.5461\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [10, 3, None, 'gini']\n",
      "Accuracy: 0.5006\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [10, 3, None, 'gini']\n",
      "Accuracy: 0.5006\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [5, 3, None, 'gini']\n",
      "Accuracy: 0.5461\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [5, 3, None, 'gini']\n",
      "Accuracy: 0.5473\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [10, 3, None, 'gini']\n",
      "Accuracy: 0.4970\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [10, 3, 'sqrt', 'gini']\n",
      "Accuracy: 0.5078\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [10, 3, 'sqrt', 'log_loss']\n",
      "Accuracy: 0.5413\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [100, 0.1]\n",
      "Accuracy: 0.5545\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [50, 0.1]\n",
      "Accuracy: 0.5473\n",
      "Iteration 50, Temperature 99.000, Best Evaluation 0.59162\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [25, 0.1]\n",
      "Accuracy: 0.5473\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [50, 0.1]\n",
      "Accuracy: 0.5473\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [50, 0.1]\n",
      "Accuracy: 0.5473\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [50, 1.0]\n",
      "Accuracy: 0.5425\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [50, 2.0]\n",
      "Accuracy: 0.5138\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [25, 2.0]\n",
      "Accuracy: 0.5078\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [25, 1.0]\n",
      "Accuracy: 0.5317\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [150, 1.0]\n",
      "Accuracy: 0.5317\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [150, 2.0]\n",
      "Accuracy: 0.5329\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [5, 3, None, 'gini']\n",
      "Accuracy: 0.5473\n",
      "Iteration 60, Temperature 99.000, Best Evaluation 0.59162\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [50, 0.5]\n",
      "Accuracy: 0.5629\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [25, 0.5]\n",
      "Accuracy: 0.5569\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [10, 0.5]\n",
      "Accuracy: 0.5545\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [10, 2.0]\n",
      "Accuracy: 0.4802\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [25, 2.0]\n",
      "Accuracy: 0.5078\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [10, 2.0]\n",
      "Accuracy: 0.4802\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [50, 2.0]\n",
      "Accuracy: 0.5138\n",
      "Generated neighborhood for algorithm: SVM, parameters: [10, 'poly', 'auto']\n",
      "Accuracy: 0.5665\n",
      "Generated neighborhood for algorithm: SVM, parameters: [10, 'poly', 'auto']\n",
      "Accuracy: 0.5665\n",
      "Generated neighborhood for algorithm: SVM, parameters: [100, 'poly', 'auto']\n",
      "Accuracy: 0.5497\n",
      "Iteration 70, Temperature 99.000, Best Evaluation 0.59162\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [100, 1.5]\n",
      "Accuracy: 0.5413\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [100, 1.0]\n",
      "Accuracy: 0.5257\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [150, 1.0]\n",
      "Accuracy: 0.5317\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [150, 1.0]\n",
      "Accuracy: 0.5317\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [150, 1.0]\n",
      "Accuracy: 0.5317\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [100, 1.0]\n",
      "Accuracy: 0.5257\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [25, 1.0]\n",
      "Accuracy: 0.5317\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [25, 0.1]\n",
      "Accuracy: 0.5473\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [150, 0.1]\n",
      "Accuracy: 0.5641\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [150, 0.5]\n",
      "Accuracy: 0.5653\n",
      "Iteration 80, Temperature 99.000, Best Evaluation 0.59162\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [150, 0.5]\n",
      "Accuracy: 0.5653\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [150, 0.5]\n",
      "Accuracy: 0.5653\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [150, 1.5]\n",
      "Accuracy: 0.5281\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [150, 0.1]\n",
      "Accuracy: 0.5641\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [150, 2.0]\n",
      "Accuracy: 0.5329\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [150, 2.0]\n",
      "Accuracy: 0.5329\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [150, 2.0]\n",
      "Accuracy: 0.5329\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [150, 0.1]\n",
      "Accuracy: 0.5641\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [150, 0.1]\n",
      "Accuracy: 0.5641\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [150, 0.1]\n",
      "Accuracy: 0.5641\n",
      "Iteration 90, Temperature 99.000, Best Evaluation 0.59162\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [150, 0.5]\n",
      "Accuracy: 0.5653\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [150, 0.5]\n",
      "Accuracy: 0.5653\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [100, 0.5]\n",
      "Accuracy: 0.5629\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [25, 0.5]\n",
      "Accuracy: 0.5569\n",
      "Generated neighborhood for algorithm: AdaBoost, parameters: [25, 0.1]\n",
      "Accuracy: 0.5473\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [15, 3, None, 'log_loss']\n",
      "Accuracy: 0.4910\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [15, 3, None, 'entropy']\n",
      "Accuracy: 0.4910\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [15, 4, None, 'entropy']\n",
      "Accuracy: 0.4934\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [15, 4, 'log2', 'entropy']\n",
      "Accuracy: 0.5293\n",
      "Generated neighborhood for algorithm: DecisionTreeClassifier, parameters: [15, 4, 'log2', 'entropy']\n",
      "Accuracy: 0.4934\n",
      "best_score is 0.5916167664670658\n",
      "best_solution is ['MLP', 3000, 'relu', 'sgd', 0.0001]\n",
      "\n",
      "Evaluating on the test data\n",
      "Test Accuracy: 0.5179\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           F       0.48      0.05      0.10       278\n",
      "           I       0.64      0.79      0.71       267\n",
      "           M       0.43      0.71      0.54       291\n",
      "\n",
      "    accuracy                           0.52       836\n",
      "   macro avg       0.52      0.52      0.45       836\n",
      "weighted avg       0.52      0.52      0.45       836\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting the AutoML algorithm\")\n",
    "automl.fit(X_train_abalone, y_train_abalone, X_val_abalone, y_val_abalone)\n",
    "\n",
    "print(\"\\nEvaluating on the test data\")\n",
    "predictions = automl.predict(X_test_abalone)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test_abalone, predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_abalone, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
